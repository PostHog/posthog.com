### Objective: Improve developer experience for creating and deploying flags

* **Rationale:**
     * Feature flags is a high friction feature. Unlike adding a snippet or simple line to track an event, engineers and product owners need to plan for their product to work with the flag on and off. 
     To fully enable engineers to build with feature flags, we should make it remarkably easy to create flags.
 
- **Key Result:** Increase in number of organizations that implement their first feature flag
- **Key Result:** Reduction in time spent between creating a feature flag and receiving the first event
    
Possible avenues:
- Auto detection of feature flags being sent that don't have a corresponding flag in posthog and easy creation (like github asking for branch to PR)
- API Enablement. Create experiments or feature flags using posthog API.
- Better hook and time to deployment with feature flag creation when someone lands on the feature flag page
- JSON flags, feature flag resiliency

### Objective: Understand similarities between users throughout posthog

* **Rationale:**
     * Help engineers stay focused on building. Give them as close to the answers and insights as possible with ample proof and dropoff points for further exploration.
 
- **Key Result:** Internal discoveries around different segments of users and groups
- **Key Result:** 80% max satisfaction rate on new features deployed

 Possible avenues:
- Cohort correlation. What are property and behavioral similarities between users in this cohort
- Experiment correlation. What are property and behavioral similarities between successful and failing variants


### Objective: Nail feature analysis

* **Rationale:**
     * We have internal power users on the growth team who actively find value in deploying with experiments. Making the act of quick insights for features seamless sets us up to deliver an opinionated workflow

- **Key Result:** Our growth team is able to ship and draw conclusions painlessly on every feature being tested
- **Key Result:** Number of experiments with significant results completed by organizations

Possible avenues:
- Complete visibility into who's seeing a flag enabled feature. See recordings connected with users
- Clarity around how users are bucketed in experiments. How do release conditions relate to control vs variants and how do variant overrides relate to those?
- Experiments UX (pushed from last Q)
