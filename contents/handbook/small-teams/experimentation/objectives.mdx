### Objective: Improve developer experience for creating and deploying flags

* **Rationale:**
     * Feature flags is a high friction feature. Unlike adding a snippet or simple line to track an event, engineers and product owners need to plan for their product to work with the flag on and off. 
     To fully enable engineers to build with feature flags, we should make it remarkably easy to create flags.
 
- **Key Result:** Double the number organizations per month that implement a feature flag
- **Key Result:** Reduction in time spent between creating a feature flag and receiving the first event
    
Possible avenues:
- Auto detection of feature flags being sent that don't have a corresponding flag in posthog and easy creation (like github asking for branch to PR)
- API Enablement. Create experiments or feature flags using posthog API.
- Better hook and time to deployment with feature flag creation when someone lands on the feature flag page
- JSON flags, feature flag resiliency

### Objective: Understand similarities between users throughout posthog

* **Rationale:**
     * Help engineers stay focused on building. Give them as close to the answers and insights as possible with ample proof and dropoff points for further exploration.
 
- **Key Result:** Actionable discoveries on cohorts that are tracked by the Posthog Team
- **Key Result:** 80% of external users give the maximum satisfaction score (:heart_eyes: ) for the new correlation features (if deployed)

 Possible avenues:
- Cohort correlation. What are property and behavioral similarities between users in this cohort
- Experiment correlation. What are property and behavioral similarities between successful and failing variants


### Objective: Nail feature analysis

* **Rationale:**
     * We have internal power users on the growth team who actively find value in deploying with experiments. Making the act of quick insights for features seamless sets us up to deliver an opinionated workflow

- **Key Result:** Internal growth team requests are primarily feature requests rather than usability and bug requests
- **Key Result:** Number of experiments with significant results completed by organizations

Possible avenues:
- Complete visibility into who's seeing a flag enabled feature. See recordings connected with users
- Clarity around how users are bucketed in experiments. How do release conditions relate to control vs variants and how do variant overrides relate to those?
- Experiments UX (pushed from last Q)
