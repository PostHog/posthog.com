---
title: OpenAI LLM analytics installation
showStepsToc: true
---

import LLMsSDKsCallout from './_snippets/llms-sdks-callout.mdx'
import VerifyLLMEventsStep from './_snippets/verify-llm-events-step.mdx'

LLM analytics is currently considered in `beta`. To access it, enable the [feature preview](https://app.posthog.com/settings/user-feature-previews#llm-observability) in your PostHog account.

<Steps>

<Step title="Install the PostHog SDK" badge="required">

Setting up analytics starts with installing the PostHog SDK for your language. LLM analytics works best with our Python and Node SDKs.

<MultiLanguage>

```bash file=Python
pip install posthog
```

```bash file=Node
npm install @posthog/ai posthog-node
```

</MultiLanguage>

</Step>

<Step title="Install the OpenAI SDK" badge="required">

Install the OpenAI SDK:

<MultiLanguage>

```bash file=Python
pip install openai
```

```bash file=Node
npm install openai
```

</MultiLanguage>

<LLMsSDKsCallout />

</Step>

<Step title="Initialize PostHog and OpenAI client" badge="required">

In the spot where you initialize the OpenAI SDK, import PostHog and our OpenAI wrapper, initialize PostHog with your project API key and host from [your project settings](https://app.posthog.com/settings/project), and pass it to our OpenAI wrapper.

<MultiLanguage>

```python
from posthog.ai.openai import OpenAI
from posthog import Posthog

posthog = Posthog(
    "<ph_project_api_key>",
    host="<ph_client_api_host>"
)

client = OpenAI(
    api_key="your_openai_api_key",
    posthog_client=posthog # This is an optional parameter. If it is not provided, a default client will be used.
)
```

```ts
import { OpenAI } from '@posthog/ai'
import { PostHog } from 'posthog-node'

const phClient = new PostHog(
  '<ph_project_api_key>',
  { host: '<ph_client_api_host>' }
);

const openai = new OpenAI({
  apiKey: 'your_openai_api_key',
  posthog: phClient,
});

// ... your code here ...

// IMPORTANT: Shutdown the client when you're done to ensure all events are sent
phClient.shutdown()
```

</MultiLanguage>

> **Note:** This also works with the `AsyncOpenAI` client. 

</Step>

<Step title="Call OpenAI LLMs" badge="required">

Now, when you use the OpenAI SDK, it automatically captures many properties into PostHog including `$ai_input`, `$ai_input_tokens`, `$ai_cache_read_input_tokens`, `$ai_latency`, `$ai_model`, `$ai_model_parameters`, `$ai_reasoning_tokens`, `$ai_tools`, `$ai_output_choices`, and `$ai_output_tokens`.

You can also capture or modify additional properties with the distinct ID, trace ID, properties, groups, and privacy mode parameters.

<MultiLanguage>

```python
response = client.responses.create(
    model="gpt-4o-mini",
    input=[
        {"role": "user", "content": "Tell me a fun fact about hedgehogs"}
    ],
    posthog_distinct_id="user_123", # optional
    posthog_trace_id="trace_123", # optional
    posthog_properties={"conversation_id": "abc123", "paid": True}, # optional
    posthog_groups={"company": "company_id_in_your_db"},  # optional 
    posthog_privacy_mode=False # optional
)

print(response.choices[0].message.content)
```


```ts
const completion = await openai.responses.create({
    model: "gpt-4o-mini",
    input: [{ role: "user", content: "Tell me a fun fact about hedgehogs" }],
    posthogDistinctId: "user_123", // optional
    posthogTraceId: "trace_123", // optional
    posthogProperties: { conversation_id: "abc123", paid: true }, // optional
    posthogGroups: { company: "company_id_in_your_db" }, // optional 
    posthogPrivacyMode: false // optional
});

console.log(completion.choices[0].message.content)
```

</MultiLanguage>

> **Notes:** 
> - We also support the old `chat.completions` API.
> - This works with responses where `stream=True`.
> - If you want to capture LLM events anonymously, **don't** pass a distinct ID to the request. See our docs on [anonymous vs identified events](/docs/data/anonymous-vs-identified-events) to learn more. 

</Step>

<VerifyLLMEventsStep />

<Step title= "Capture embeddings" badge="optional">

PostHog can also capture embedding generations as `$ai_embedding` events. Just make sure to use the same `posthog.ai.openai` client to do so:

```python
response = client.embeddings.create(
    input="The quick brown fox",
    model="text-embedding-3-small",
    posthog_distinct_id="user_123", # optional
    posthog_trace_id="trace_123",   # optional
    posthog_properties={"key": "value"} # optional
    posthog_groups={"company": "company_id_in_your_db"}  # optional 
    posthog_privacy_mode=False # optional
)
```

</Step>

</Steps>