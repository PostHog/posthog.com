---
title: LiteLLM analytics installation
showStepsToc: true
---

import LLMsSDKsCallout from './_snippets/llms-sdks-callout.mdx'
import VerifyLLMEventsStep from './_snippets/verify-llm-events-step.mdx'
import NotableGenerationProperties from '../_snippets/notable-generation-properties.mdx'

> **Note:** LiteLLM can be used as a Python SDK or as a proxy server.

<Steps>

<Step title="Install LiteLLM" badge="required">

Choose your installation method based on how you want to use LiteLLM:

<MultiLanguage>

```bash file=SDK
pip install litellm
```

```bash file=Proxy
# Install via pip
pip install 'litellm[proxy]'

# Or run via Docker
docker run --rm -p 4000:4000 ghcr.io/berriai/litellm:latest
```

</MultiLanguage>

</Step>

<Step title="Configure PostHog observability" badge="required">

Configure PostHog by setting your API key and enabling observability. You can find your API key in [your project settings](https://app.posthog.com/settings/project).

<MultiLanguage>

```python file=SDK
import os
import litellm

# Set environment variables
os.environ["POSTHOG_API_KEY"] = "<ph_project_api_key>"
os.environ["POSTHOG_API_URL"] = "<ph_client_api_host>"  # Optional, defaults to https://app.posthog.com

# Enable PostHog callbacks
litellm.success_callback = ["posthog"]
litellm.failure_callback = ["posthog"]  # Optional: also log failures
```

```yaml file=Proxy
# config.yaml
model_list:
- model_name: gpt-4o-mini
  litellm_params:
    model: gpt-4o-mini

litellm_settings:
  success_callback: ["posthog"]
  failure_callback: ["posthog"]  # Optional: also log failures

environment_variables:
  POSTHOG_API_KEY: "<ph_project_api_key>"
  POSTHOG_API_URL: "<ph_client_api_host>"  # Optional
```

</MultiLanguage>

</Step>

<Step title="Call LLMs through LiteLLM" badge="required">

Now, when you use LiteLLM to call various LLM providers, PostHog automatically captures an `$ai_generation` event.

<MultiLanguage>

```python file=SDK
response = litellm.completion(
    model="gpt-4o-mini",
    messages=[
        {"role": "user", "content": "Tell me a fun fact about hedgehogs"}
    ],
    metadata={
        "user_id": "user_123",  # Maps to PostHog distinct_id
        "company": "company_id_in_your_db"  # Custom property
    }
)

print(response.choices[0].message.content)
```

```bash file=Proxy
# Start the proxy (if not already running)
litellm --config config.yaml

# Make a request to the proxy
curl -X POST http://localhost:4000/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [
      {"role": "user", "content": "Tell me a fun fact about hedgehogs"}
    ],
    "metadata": {
      "user_id": "user_123",
      "company": "company_id_in_your_db" # Custom property
    }
  }'
```

</MultiLanguage>

> **Notes:** 
> - This works with streaming responses by setting `stream=True`.
> - To disable logging for specific requests, add `{"no-log": true}` to metadata.
> - If you want to capture LLM events anonymously, **don't** pass a `user_id` in metadata. See our docs on [anonymous vs identified events](/docs/data/anonymous-vs-identified-events) to learn more.

You can expect captured `$ai_generation` events to have the following properties:

<NotableGenerationProperties />

</Step>

<VerifyLLMEventsStep />

<Step title="Capture embeddings" badge="optional">

PostHog can also capture embedding generations as `$ai_embedding` events through LiteLLM:

<MultiLanguage>

```python file=SDK
response = litellm.embedding(
    input="The quick brown fox",
    model="text-embedding-3-small",
    metadata={
        "user_id": "user_123",  # Maps to PostHog distinct_id
        "company": "company_id_in_your_db"  # Custom property
    }
)
```

```bash file=Proxy
# Make an embeddings request to the proxy
curl -X POST http://localhost:4000/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "input": "The quick brown fox",
    "model": "text-embedding-3-small",
    "metadata": {
      "user_id": "user_123",
      "company": "company_id_in_your_db" # Custom property
    }
  }'
```

</MultiLanguage>

</Step>

</Steps>