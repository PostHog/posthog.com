---
title: Getting started with LLM analytics
hideAnchor: true
---

import { QuestLog, QuestLogItem } from 'components/Docs/QuestLog'
import { IconGraph, IconWarning, IconRewindPlay  } from '@posthog/icons'
import LLMsInstallationPlatforms from './_snippets/llms-installation-platforms.tsx'


<QuestLog firstSpeechBubble="Let's get started!" lastSpeechBubble="Time to integrate LLM analytics!">

<QuestLogItem 
    title="Capture LLM conversations"
    subtitle="Required"
    icon="IconLlmPromptManagement"
>

  LLM analytics gives you x-ray vision into your LLM applications. You can track:

  - üó£Ô∏è Every conversation (inputs, outputs, and tokens)
  - ü§ñ Model performance (cost, latency and error rates)
  - üîç Full traces for when you need to go detective mode
  - üí∞ How much each chat/user/organization is costing you

  The first step is to install a PostHog SDK to capture conversations, requests, and responses from an LLM provider.

  ### Platforms

  <LLMsInstallationPlatforms />

  ### Beta üöß

  LLM analytics is currently considered in `beta`. To access it, enable the [feature preview](https://app.posthog.com/settings/user-feature-previews#llm-observability) in your PostHog account.
  
  We are keen to gather as much feedback as possible so if you try this out please let us know. You can email [peter@posthog.com](mailto:peter@posthog.com) and [radu@posthog.com](mailto:radu@posthog.com), send feedback via the [in-app support panel](https://us.posthog.com#panel=support%3Afeedback%3Aexperiments%3Alow), or use one of our other [support options](/docs/support-options).

  <CallToAction type="primary" to="/docs/llm-analytics/installation">
    Install PostHog SDK
  </CallToAction>

</QuestLogItem>

<QuestLogItem 
  title="Record generations"
  subtitle="Required"
  icon="IconRecord"
>

  Once you've installed the SDK, every LLM call automatically becomes a [generation](/docs/llm-analytics/generations) ‚Äì a detailed record of what went in and what came out. Each generation captures:

  - üìù Complete conversation context (inputs and outputs)
  - üî¢ Token counts and usage metrics  
  - ‚è±Ô∏è Response latency and performance data
  - üí∏ Automatic cost calculation based on model pricing
  - üîó Trace IDs to group related LLM calls together

PostHog's SDK wrappers handle all the heavy lifting. Use your LLM provider as normal and we'll capture everything automatically. 
  
You can also add custom properties like user IDs, conversation IDs, or feature flags to understand usage patterns.

  <CallToAction type="primary" to="/docs/llm-analytics/generations">
    Learn about generations
  </CallToAction>

</QuestLogItem>

<QuestLogItem 
  title="Analyze model usage"
  subtitle="Required"
  icon="IconLineGraph"
>

  PostHog's LLM analytics dashboard provides a comprehensive overview of your LLM performance. Break usage metrics down by model, latency, cost, and more.
  
  <ProductScreenshot
    imageLight="https://res.cloudinary.com/dmukukwp6/image/upload/Clean_Shot_2025_01_15_at_08_31_29_4e1702243d.png"
    imageDark="https://res.cloudinary.com/dmukukwp6/image/upload/Clean_Shot_2025_01_15_at_08_31_11_66aa4e13b7.png"
    alt="LLM dashboard"
    classes="rounded"
/>

  <CallToAction type="primary" to="/docs/llm-analytics/dashboard">
    Anyalze LLM performance
  </CallToAction>

</QuestLogItem>

<QuestLogItem 
  title="Integrate customer data"
  subtitle="Recommended"
  icon="IconLogomark"
>

  Take advantage of PostHog's [platform](/docs) to integrate your customer data with LLM analytics.

  ### <IconGraph className="text-blue w-7 -mt-1 inline-block"/>  Product analytics

  All LLM analytics are captured as standard PostHog events, which means you can create dashboards, trends, funnels, custom SQL queries, alerts, and more. 

  <ProductScreenshot
    imageLight="https://res.cloudinary.com/dmukukwp6/image/upload/llma_insights_da40edc407.png"
    imageDark="https://res.cloudinary.com/dmukukwp6/image/upload/llma_insights_dark_558f8f2cd8.png"
    alt="LLM observability dashboard"
    classes="rounded"
/>

  ### <IconWarning className="text-orange w-7 -mt-1 inline-block"/>  Error tracking

  LLM generated errors are automatically captured in PostHog's [error tracking](/docs/error-tracking) for you to monitor, debug, and resolve.

  <ProductScreenshot
    imageLight="https://res.cloudinary.com/dmukukwp6/image/upload/llma_error_4edcb7d7a1.png"
    imageDark="https://res.cloudinary.com/dmukukwp6/image/upload/llma_error_dark_a298d3f2b7.png"
    alt="LLM analytics error tracking"
    classes="rounded"
/>

  ### <IconRewindPlay className="text-yellow w-7 -mt-1 inline-block"/> Session replay
  
  Watch [session replays](/docs/feature-flags) to see exactly how users interact with your LLM features.

  <ProductScreenshot
    imageLight="https://res.cloudinary.com/dmukukwp6/image/upload/llma_session_replay_95b9268668.png"
    imageDark="https://res.cloudinary.com/dmukukwp6/image/upload/llma_session_replay_dark_767332d926.png"
    alt="LLM analytics session replay"
    classes="rounded"
/>

</QuestLogItem>

<QuestLogItem 
  title="Use for free"
  subtitle="Open beta"
  icon="IconPiggyBank"
>

  LLM analytics is currently in beta, with events priced the same as regular PostHog data events, which comes with a generous free tier and transparent usage-based pricing. 

  No credit card required to start. To access LLM analytics, enable the [feature preview](https://app.posthog.com/settings/user-feature-previews#llm-observability) in your PostHog account. 

  --- 

  That's it! You're ready to start integrating.

  <CallToAction type="primary" to="/docs/llm-analytics/installation">
    Install LLM analytics
  </CallToAction>

</QuestLogItem>

</QuestLog>
