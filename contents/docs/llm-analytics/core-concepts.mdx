---
title: LLM Analytics core concepts
---

import OSButton from 'components/OSButton'

This page covers the foundational concepts you'll encounter throughout the LLM Analytics docs.

## What is an event?

An **event** is the core unit of data in PostHog. It represents an interaction a user has with your product, like making an LLM call. When you make an LLM call, PostHog captures it as an event with properties like the model used, tokens consumed, latency, and cost.

You can query, filter, and visualize these events to understand how your LLM features are performing.

## How LLM Analytics uses events

LLM Analytics uses the following core concepts to capture your AI application's activity.

### Generations

A **generation** is a single call to an LLM where the model generates a response based on your input. When you send a prompt to GPT-4, Claude, or Gemini and get a response back â€“ that's one generation.

<OSButton variant="secondary" to="/docs/llm-analytics/generations">Learn more about generations</OSButton>

### Traces

A **trace** groups related LLM operations together. When a user asks your AI assistant a question, you might retrieve documents, call an LLM, and log the conversation. Trace groups all these steps under one ID.

<OSButton variant="secondary" to="/docs/llm-analytics/traces">Learn more about traces</OSButton>

### Spans

A **span** represents a single operation within a trace, like a database search, a tool call, or a data transformation. Spans can be nested to show how operations relate to each other.

<OSButton variant="secondary" to="/docs/llm-analytics/spans">Learn more about spans</OSButton>

### Embeddings

**Embeddings** convert text into vectors that capture meaning. Similar texts produce similar embeddings, enabling semantic search and RAG (retrieval-augmented generation) pipelines.

<OSButton variant="secondary" to="/docs/llm-analytics/embeddings">Learn more about embeddings</OSButton>

### Sessions

A **session** groups multiple traces together. Use sessions to track a user's conversation thread, a workflow, or any logical grouping that spans multiple LLM interactions.

<OSButton variant="secondary" to="/docs/llm-analytics/sessions">Learn more about sessions</OSButton>

## Tokens and costs

### What are tokens?

**Tokens** are the units LLMs use to process text.

LLM providers charge based on token usage:

- **Input tokens**: The tokens in your prompt
- **Output tokens**: The tokens the model generates in response

Tracking tokens helps you understand costs and optimize prompts.

### Providers and models

- **Provider**: The company that hosts the LLM (e.g., OpenAI, Anthropic, Google)
- **Model**: The specific AI model you're calling (e.g., `gpt-4o`, `claude-sonnet-4`, `gemini-pro`)

One provider can offer multiple models. For example, OpenAI offers GPT-4o, GPT-4o-mini, and o1.

## Message roles

LLM APIs use **roles** to distinguish between different types of messages in a conversation:

| Role | Description |
|------|-------------|
| **system** | Instructions that set the assistant's behavior, like "You are a helpful assistant" |
| **user** | Messages from the end user asking questions or making requests |
| **assistant** | Previous responses from the LLM, used for conversation history |

Understanding roles helps you debug prompts and see exactly what was sent to the model.
