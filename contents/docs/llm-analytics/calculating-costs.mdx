---
title: Calculating LLM costs
---

## How are LLM costs calculated?

PostHog calculates cost based on the number of input (prompt) and output (completion) tokens generated by specific AI models, plus additional cost components like per-request and per-web-search pricing when applicable.

### Model matching

We use OpenRouter's pricing data as our primary source to match your model and calculate costs. OpenRouter provides comprehensive pricing information for models across different providers, and we use both the `$ai_provider` and `$ai_model` properties from your events to find the exact pricing.

If OpenRouter doesn't have pricing data for a specific model, we fall back to our manually maintained pricing database for additional model support.

### Cost components

The total cost (`$ai_total_cost_usd`) is calculated from multiple components:

- **Token-based costs**: Input (prompt) and output (completion) tokens
- **Cache costs**: Cached token reads and writes (when applicable)
- **Request-based costs**: Some models charge per request in addition to token costs
- **Web search costs**: Some models charge per web search performed

For cached LLM responses, our pricing models include cached token pricing which we automatically apply.

We also take into account the reasoning / thinking tokens for models that support it.

## Setting custom pricing

You can override PostHog's automatic cost calculation by providing custom pricing for your LLM models. This is useful when:

- You have negotiated custom pricing with your LLM provider
- You're using a model that PostHog doesn't support yet
- PostHog's automatic pricing doesn't match your specific use case

### Option 1: Custom price per token

If you know your pricing per token, you can set the following [custom properties](/docs/llm-analytics/custom-properties) when calling your LLM:

- `$ai_input_token_price` (required): Price per input/prompt token
- `$ai_output_token_price` (required): Price per output/completion token
- `$ai_cache_read_token_price` (optional): Price per cached token read
- `$ai_cache_write_token_price` (optional): Price per cached token write
- `$ai_request_price` (optional): Price per request (for models that charge per request)
- `$ai_web_search_price` (optional): Price per web search (for models that charge per search)

**Important:** Token prices should be per individual token, not per million tokens. For example, if your provider charges $0.03 per 1M tokens, you would set `$ai_input_token_price: 0.00000003` (0.03 / 1,000,000).

<MultiLanguage syncKey="sdk-backend">

```node
import { OpenAI } from '@posthog/ai'
import { PostHog } from 'posthog-node'

const phClient = new PostHog(
  '<ph_project_api_key>',
  { host: '<ph_client_api_host>' }
)

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
  posthog: phClient
})

const response = await openai.responses.create({
  model: 'my-custom-model',
  messages: [{ role: 'user', content: 'Hello' }],
  posthogProperties: {
    $ai_input_token_price: 0.00000003,   // $0.03 per 1M tokens = $0.00000003 per token
    $ai_output_token_price: 0.00000006,  // $0.06 per 1M tokens = $0.00000006 per token
    // Optional: cache pricing
    $ai_cache_read_token_price: 0.000000015,
    $ai_cache_write_token_price: 0.0000000375
  }
})
```

```python
from posthog.ai.openai import OpenAI
from posthog import Posthog

posthog = Posthog(
    "<ph_project_api_key>",
    host="<ph_client_api_host>"
)

client = OpenAI(
    api_key="sk-...",
    posthog_client=posthog
)

response = client.responses.create(
    model="my-custom-model",
    messages=[{"role": "user", "content": "Hello"}],
    posthog_properties={
        "$ai_input_token_price": 0.00000003,   # $0.03 per 1M tokens = $0.00000003 per token
        "$ai_output_token_price": 0.00000006,  # $0.06 per 1M tokens = $0.00000006 per token
        # Optional: cache pricing
        "$ai_cache_read_token_price": 0.000000015,
        "$ai_cache_write_token_price": 0.0000000375
    }
)
```

</MultiLanguage>

Both `$ai_input_token_price` and `$ai_output_token_price` must be provided for custom pricing to take effect. PostHog will then calculate the total cost based on the token counts and your custom prices.

### Option 2: Pre-calculated costs

If you're [manually capturing](/docs/llm-analytics/installation/manual-capture) LLM events and have already calculated the total costs yourself, you can send them directly:

- `$ai_input_cost_usd`: Total cost for input/prompt tokens in USD
- `$ai_output_cost_usd`: Total cost for output/completion tokens in USD
- `$ai_request_cost_usd`: Total cost for requests in USD
- `$ai_web_search_cost_usd`: Total cost for web searches in USD

<MultiLanguage syncKey="sdk-backend">

```node
// After making your LLM call and calculating costs
posthog.capture('$ai_generation', {
  $ai_trace_id: traceId,
  $ai_model: 'my-custom-model',
  $ai_provider: 'my-provider',
  $ai_input_tokens: inputTokens,
  $ai_output_tokens: outputTokens,
  $ai_input_cost_usd: 0.0042,
  $ai_output_cost_usd: 0.0028
  // ... other required properties
})
```

```python
# After making your LLM call and calculating costs
posthog.capture(
    event='$ai_generation',
    properties={
        '$ai_trace_id': trace_id,
        '$ai_model': 'my-custom-model',
        '$ai_provider': 'my-provider',
        '$ai_input_tokens': input_tokens,
        '$ai_output_tokens': output_tokens,
        '$ai_input_cost_usd': 0.0042,
        '$ai_output_cost_usd': 0.0028
        # ... other required properties
    }
)
```

</MultiLanguage>

PostHog will use these values directly without any additional calculation.

### Precedence

Cost calculation follows this precedence order:

1. **Pre-calculated costs** (`$ai_input_cost_usd`, `$ai_output_cost_usd`, `$ai_request_cost_usd`, `$ai_web_search_cost_usd`): These values are used directly without any additional calculation
2. **Custom pricing** (`$ai_input_token_price`, `$ai_output_token_price`, `$ai_request_price`, `$ai_web_search_price`): PostHog calculates costs from token counts and usage metrics using your custom prices
3. **Automatic model matching**: PostHog matches your model using OpenRouter's pricing data first, then falls back to manually maintained pricing if needed

You can find the code for this on [GitHub](https://github.com/PostHog/posthog/tree/master/plugin-server/src/ingestion/ai-costs).
