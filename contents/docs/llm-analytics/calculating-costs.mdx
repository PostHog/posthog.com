---
title: Calculating LLM costs
---

## How are LLM costs calculated?

PostHog calculates cost based on the number of input (prompt) and output (completion) tokens generated by specific AI models.

To determine the pricing for a model, we use a two-step matching process:

1. **Primary matching**: We use both `$ai_provider` and `$ai_model` properties from your events to find the exact pricing for a model at a specific provider. This allows us to account for price variations across different providers for the same model.

2. **Fallback matching**: If we can't find pricing data for a specific provider-model combination, we fall back to OpenRouter's pricing data. OpenRouter provides general pricing information for models without provider-specific breakdowns, which we use as a default when exact provider pricing isn't available. 

For cached LLM responses, our pricing models include cached token pricing which we automatically apply.

We also take into account the reasoning / thinking tokens for models that support it.
 
You can find the code for this on [GitHub](https://github.com/PostHog/posthog/tree/master/plugin-server/src/ingestion/ai-costs).
