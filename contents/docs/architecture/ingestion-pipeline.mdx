---
title: Ingestion pipeline
---

In it's simplest form, PostHog is an analytics data store where events come in and get analyzed.

This document gives an overview of how data ingestion works.

## Capture API

Before an event reaches the Ingestion pipeline, there are a couple of preliminary checks that we do so that we can return a response immediately to the client.
These checks are performed as part of the Capture API, and consist of:

-   Validating API keys.
-   Anonymizing IPs according to project settings.
-   Decompressing and normalizing the shape of event data for the rest of the system.
-   Sending processed to `events_plugin_ingestion` Kafka topic.
-   If communication with Postgres fails, logging events to kafka `dead_letter_queue` table.

The goal of this step is to be as simple as possible, so that we can reliably get events into the ingestion pipeline, where Kafka can persist them unti they are able to be processed.

## Ingestion

Due to the design of Clickhouse, update operations are slow and undesireable, and so the first 4 steps of the ingestion pipeline are performing operations on our raw events to get it ready to be persisted in Clickhouse.

The ingestion pipeline is made up of a number of different step:

1. Event buffer
2. Apps - `processEvent`
3. Person processing
4. Event processing
5. Writing to Clickhouse
6. Apps - `onEvent`

### 1. Event buffer

The event buffer sits right at the beginning of the ingestion pipeline, and gives us the ability to selectively delay the processing of certain events.
Events are initially read from the `events_plugin_ingestion` Kafka topic, which is written to by the Capture API.

#### Why are events buffered?

It will become more obvious why this is necessary as we dive deeper into the event pipeline, but at a high level it's meant to alleviate some edge cases that come with how we process events.
In order to make sure that our queries run as fast as possible, Clickhouse

#### Determining if an event should be buffered

After an event is conusmed from Kafka, the plugin server will check a number of things in order to determine whether or not to buffer an event.
If any of these conditions are true the event will not be buffered and will immediately move on to the [next step](#2-apps---processevent).

-   If the `distinct_id` on the event is already associated with an existing Person
-   If the event is _anonymous_ (In this case this means the `distinct_id` matches the `device_id` generated by the library that sent the event)
-   If the event is an `$identify` call
-   If the event is coming from one of our mobile libraries, as it is not easy to determine if an event is _anonymous_ and it is not worth it to buffer all events on mobile

If an event coming in satisfies none of these checks, then it will be added to the buffer and sent back through the ingestion pipeline in _at most_ 60s.

For more detail on this step, check out [this file](https://github.com/PostHog/posthog/blob/master/plugin-server/src/worker/ingestion/event-pipeline/1-emitToBufferStep.ts) from the plugin-server codebase.

### 2. Apps - `processEvent`

After the event buffer, we start the first of a few steps that augment or transform our raw event before it gets written into Clickhouse.
This first step runs any workloads that come from [Apps](/apps) that you have installed and who have exported a `processEvent` function.
This is the only chance for apps to transform or exclude an event before it is writtend into Clickhouse.

For more detail on this step, check out [this file](https://github.com/PostHog/posthog/blob/master/plugin-server/src/worker/ingestion/event-pipeline/2-pluginsProcessEventStep.ts) from the plugin-server codebase.

### 3. Person processing

The next step in the ingestion pipeline is processing the Person who sent the event.
This Person is determined by the `distinct_id` field, and a number of different actions can happend here depending both on if we've seen this `distinct_id` before, as well as which type of event is being sent.

To determine whether or not a Person and corresponding `person_id` have already been created and associated with a `distinct_id`, we make a call to PostgreSQL to retrieve the latest mappings.

#### `$identify` or `$alias` event

|                                                                                                                                                 |                                                                                                              |
| ----------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |
| If we have never seen this `distinct_id` before                                                                                                 | Create a new Person and add a mapping in PostgreSQL to associate this `distinct_id` with the new `person_id` |
| If there is a single Person already associated with this user                                                                                   | Create a new mapping to associate this `distinct_id` with the already existing `person_id`                   |
| If there is already a Person associated with this `distinct_id` _and_ we have already created a second Person that is associated with this user | We will [merge these two people](#merging-two-persons) and associate all future events with the first Person |

##### Merging two Persons

If we come across the scenario where we have inadvertently created two Persons for the same user, we will need to merge these two Persons.
This scenario typically isn't ideal, as merging two users will only affect events that are sent in the future, and as such there will be one Person who is left with events from the past even though we know that they are the same person.
This comes as a consequence of not easily being able to efficiently update events once they are written to the database, as the only way to fully 'merge' these two people would be to go back and rewrite the `person_id` field on past events.
Avoiding this use case is the primary goal of the event buffer, as by buffering events from users we have never seen before, we can avoid creating a duplicate Person for this new user if it turns out that they have been identified or logged in shortly after sending their first event.

#### All other events

### 4. Event processing

Finally, before we write the event to Clickhouse we perform a few last processing steps.
This is our last chance to change anything about the event, which can include:

-   Adding group properties if the event has been assigned to a Group
-   Anonymizing IPs, if needed
-   Attaching the `person_id` for the Person who sent the event, this was determined in Step 3
-   Attach person properties to the event

### 5. Writing to Clickhouse

Now that we have our fully-processed event with all the person properties attached, we are ready to persist it by writing it to Clickhouse.
This is done by writing our finalized event to a separate Kafka topic that Clickhouse will consume-from and then write to its tables.

For more information on exactly how events are stored in Clickhouse, check out [this reference](/docs/architecture/clickhouse)

### 6. Apps - `onEvent`

The final step in the ingestion pipeline is calling the `onEvent` handler from any apps that we have enabled. This includes all of our export apps as well as some of our alerting/monitoring apps.
It's worth noting that since this event has already been written to Clickhouse, it is effectively immutable at this point as we do not allow apps to directly update events. Any apps that need to tranform events should use the `processEvent` handler.

## Kafka

Kafka is used as a resilient message bus between different services.

You can find relevant kafka topics [in the PostHog codebase](https://github.com/PostHog/posthog/blob/master/posthog/kafka_client/topics.py).
