---
title: Getting started with LLM observability
hideAnchor: true
---

import { QuestLog, QuestLogItem } from 'components/Docs/QuestLog'
import { IconGraph, IconWarning, IconToggle  } from '@posthog/icons'
import LLMsInstallationPlatforms from './_snippets/llms-installation-platforms.tsx'


<QuestLog firstSpeechBubble="Let's get started!" lastSpeechBubble="Time to integrate LLM observability!">

<QuestLogItem 
    title="Capture LLM conversations"
    subtitle="Required"
    icon="IconLlmPromptManagement"
>

  LLM observability gives you x-ray vision into your LLM applications. You can track:

  - ğŸ—£ï¸ Every conversation (inputs, outputs, and tokens)
  - ğŸ¤– Model performance (cost, latency and error rates)
  - ğŸ” Full traces for when you need to go detective mode
  - ğŸ’° How much each chat/user/organization is costing you


  The first step is to install a PostHog SDK to capture conversations, requests, and responses from an LLM provider.

  ### Platforms

  <LLMsInstallationPlatforms />

  ### Beta ğŸš§

  LLM observability is currently considered in `beta`. To access it, enable the [feature preview](https://app.posthog.com/settings/user-feature-previews#llm-observability) in your PostHog account.
  
  We are keen to gather as much feedback as possible so if you try this out please let us know. You can email [peter@posthog.com](mailto:peter@posthog.com) and [radu@posthog.com](mailto:radu@posthog.com), send feedback via the [in-app support panel](https://us.posthog.com#panel=support%3Afeedback%3Aexperiments%3Alow), or use one of our other [support options](/docs/support-options).

  <CallToAction type="primary" to="/docs/llm-observability/installation">
    Install PostHog SDK
  </CallToAction>

</QuestLogItem>

<QuestLogItem 
  title="Record generations"
  subtitle="Required"
  icon="IconRecord"
>

  Once you've installed the SDK, every LLM call automatically becomes a **generation** â€“ a detailed record of what went in and what came out. Each generation captures:

  - ğŸ“ Complete conversation context (inputs and outputs)
  - ğŸ”¢ Token counts and usage metrics  
  - â±ï¸ Response latency and performance data
  - ğŸ’¸ Automatic cost calculation based on model pricing
  - ğŸ”— Trace IDs to group related LLM calls together


  PostHog's SDK wrappers handle all the heavy lifting â€“ just use your LLM provider as normal and we'll capture everything automatically. You can also add custom properties like user IDs, conversation IDs, or feature flags to understand usage patterns.

  <CallToAction type="primary" to="/docs/llm-observability/generations">
    Learn more
  </CallToAction>

</QuestLogItem>

<QuestLogItem 
  title="Integrate customer data"
  subtitle="Recommended"
  icon="IconLogomark"
>

  Take advantage of PostHog's [platform](/docs) to integrate your customer data with LLM observability.

  ### <IconGraph className="text-blue w-7 -mt-1 inline-block"/>  Product analytics

  The best part? All this data gets sent as regular PostHog events, where you can slice, dice, and analyze it in dashboards, insights, and alerts.

  content tk...

  ### <IconWarning className="text-orange w-7 -mt-1 inline-block"/>  Error tracking
  
  content tk...

  ### <IconToggle className="text-seagreen w-7 -mt-1 inline-block"/>  Feature flags
  
  content tk...

</QuestLogItem>

<QuestLogItem 
  title="Use for free"
  subtitle="Free 1M events/mo"
  icon="IconPiggyBank"
>

  PostHog's LLM observability is built to be cost-effective. With a generous free tier and transparent usage-based pricing, more than 90% of companies use PostHog completely for free. 
  
  LLM observability events priced the same as regular PostHog data events, making it roughly **10x cheaper** than other LLM observability tools.

  ### TL;DR ğŸ’¸ 

  - No credit card required to start
  - Free 1M events per month are free
  - Above 100k we have usage-based pricing starting at $0.00005/event with discounts as volume increases
  - Set billing limits to avoid surprise charges
  - See our [pricing page](/pricing) for more up-to-date details


  --- 

  That's it! You're ready to start integrating.

  <CallToAction type="primary" to="/docs/llm-observability/installation">
    Install LLM observability
  </CallToAction>

</QuestLogItem>

</QuestLog>
