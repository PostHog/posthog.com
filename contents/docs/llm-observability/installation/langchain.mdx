---
title: LangChain observability installation
showStepsToc: true
---

import LLMsSDKsCallout from './_snippets/llms-sdks-callout.mdx'
import VerifyLLMEventsStep from './_snippets/verify-llm-events-step.mdx'

LLM observability is currently considered in `beta`. To access it, enable the [feature preview](https://app.posthog.com/settings/user-feature-previews#llm-observability) in your PostHog account.

<Steps>

<Step title="Install the PostHog SDK" badge="required">

Setting up observability starts with installing the PostHog SDK for your language. LLM observability works best with our Python and Node SDKs.

<MultiLanguage>

```bash file=Python
pip install posthog
```

```bash file=TypeScript
npm install @posthog/ai posthog-node
```

</MultiLanguage>

</Step>

<Step title="Install LangChain and OpenAI SDKs" badge="required">

Install the LangChain and OpenAI Python SDKs:

<MultiLanguage>

```bash file=Python
pip install langchain openai langchain-openai
```

```bash file=TypeScript
npm install langchain @langchain/core @posthog/ai
```

</MultiLanguage>

<LLMsSDKsCallout />

</Step>

<Step title="Initialize PostHog and LangChain" badge="required">

In the spot where you make your OpenAI calls, import PostHog, LangChain, and our LangChain `CallbackHandler`. Initialize PostHog with your project API key and host (from [your project settings](https://us.posthog.com/settings/project)), and pass it to the `CallbackHandler` along with (optionally) a user distinct ID, trace ID, PostHog properties, [groups](/docs/product-analytics/group-analytics), and privacy mode.

<MultiLanguage>

```python
from posthog.ai.langchain import CallbackHandler
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from posthog import Posthog

posthog = Posthog(
    "<ph_project_api_key>",
    host="<ph_client_api_host>"
)

callback_handler = CallbackHandler(
    client=posthog, # This is an optional parameter. If it is not provided, a default client will be used.
    distinct_id="user_123", # optional
    trace_id="trace_456", # optional
    properties={"conversation_id": "abc123"} # optional
    groups={"company": "company_id_in_your_db"} # optional
    privacy_mode=False # optional
)
```

```typescript
import { PostHog } from 'posthog-node';
import { LangChainCallbackHandler } from '@posthog/ai';
import { ChatOpenAI } from '@langchain/openai';
import { ChatPromptTemplate } from '@langchain/core/prompts';

const phClient = new PostHog(
  '<ph_project_api_key>',
  { host: '<ph_client_api_host>' }
);

const callbackHandler = new LangChainCallbackHandler({
  client: phClient,
  distinctId: 'user_123', // optional
  traceId: 'trace_456', // optional
  properties: { conversationId: 'abc123' }, // optional
  groups: { company: 'company_id_in_your_db' }, // optional
  privacyMode: false, // optional
  debug: false // optional - when true, logs all events to console
});
```

</MultiLanguage>

> **Note:** If you want to capture LLM events anonymously, **don't** pass a distinct ID to the `CallbackHandler`. See our docs on [anonymous vs identified events](/docs/data/anonymous-vs-identified-events) to learn more. 

</Step>

<Step title="Make LangChain calls" badge="required">

When you invoke your chain, pass the `callback_handler` in the `config` as part of your `callbacks`:

<MultiLanguage>

```python
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    ("user", "{input}")
])

model = ChatOpenAI(openai_api_key="your_openai_api_key")
chain = prompt | model

# Execute the chain with the callback handler
response = chain.invoke(
    {"input": "Tell me a joke about programming"},
    config={"callbacks": [callback_handler]}
)

print(response.content)
```

```typescript
const prompt = ChatPromptTemplate.fromMessages([
  ["system", "You are a helpful assistant."],
  ["user", "{input}"]
]);

const model = new ChatOpenAI({
  apiKey: "your_openai_api_key"
});

const chain = prompt.pipe(model);

// Execute the chain with the callback handler
const response = await chain.invoke(
  { input: "Tell me a joke about programming" },
  { callbacks: [callbackHandler] }
);

console.log(response.content);
phClient.shutdown();
```

</MultiLanguage>

This automatically captures many properties into PostHog including `$ai_input`, `$ai_input_tokens`, `$ai_latency`, `$ai_model`, `$ai_model_parameters`, `$ai_output_choices`, and `$ai_output_tokens`. It also automatically creates a trace hierarchy based on how LangChain components are nested.

</Step>

<VerifyLLMEventsStep />

</Steps>