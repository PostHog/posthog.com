Start by installing the OpenAI SDK:

<MultiLanguage>

```bash file=Python
pip install openai
```

```bash file=TypeScript
npm install openai
```

</MultiLanguage>

In the spot where you initialize the OpenAI SDK, import PostHog and our OpenAI wrapper, initialize PostHog with your project API key and host (from [your project settings](https://us.posthog.com/settings/project)), and pass it to our OpenAI wrapper.

<MultiLanguage>

```python
from posthog.ai.openai import OpenAI
import posthog

posthog.project_api_key = "<ph_project_api_key>"
posthog.host = "<ph_client_api_host>"

client = OpenAI(
    api_key="your_openai_api_key",
    posthog_client=posthog
)
```

```ts
import { OpenAI } from '@posthog/ai'
import { PostHog } from 'posthog-node'

const phClient = new PostHog(
  '<ph_project_api_key>',
  { host: '<ph_client_api_host>' }
);

const openai = new OpenAI({
  apiKey: 'your_openai_api_key',
  posthog: phClient,
});

// ... your code here ...

// IMPORTANT: Shutdown the client when you're done to ensure all events are sent
phClient.shutdown()
```

</MultiLanguage>

> **Note:** This also works with the `AsyncOpenAI` client. 

Now, when you use the OpenAI SDK, it automatically captures many properties into PostHog including `$ai_input`, `$ai_input_tokens`, `$ai_latency`, `$ai_model`, `$ai_model_parameters`, `$ai_output_choices`, and `$ai_output_tokens`.

You can also capture or modify additional properties with the distinct ID, trace ID, properties, groups, and privacy mode parameters.

<MultiLanguage>

```python
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "user", "content": "Tell me a fun fact about hedgehogs"}
    ],
    posthog_distinct_id="user_123", # optional
    posthog_trace_id="trace_123", # optional
    posthog_properties={"conversation_id": "abc123", "paid": True}, # optional
    posthog_groups={"company": "company_id_in_your_db"},  # optional 
    posthog_privacy_mode=False # optional
)

print(response.choices[0].message.content)
```

```ts
const completion = await openai.chat.completions.create({
    model: "gpt-3.5-turbo",
    messages: [{ role: "user", content: "Tell me a fun fact about hedgehogs" }],
    posthogDistinctId: "user_123", // optional
    posthogTraceId: "trace_123", // optional
    posthogProperties: { conversation_id: "abc123", paid: true }, // optional
    posthogGroups: { company: "company_id_in_your_db" }, // optional 
    posthogPrivacyMode: false // optional
});

console.log(completion.choices[0].message.content)
```

</MultiLanguage>

> **Notes:** 
> - This also works with responses where `stream=True`.
> - If you want to capture LLM events anonymously, **don't** pass a distinct ID to the request. See our docs on [anonymous vs identified events](/docs/data/anonymous-vs-identified-events) to learn more. 

### Embeddings

PostHog can also capture embedding generations as `$ai_embedding` events. Just make sure to use the same `posthog.ai.openai` client to do so:

```python
response = client.embeddings.create(
    input="The quick brown fox",
    model="text-embedding-3-small",
    posthog_distinct_id="user_123", # optional
    posthog_trace_id="trace_123",   # optional
    posthog_properties={"key": "value"} # optional
    posthog_groups={"company": "company_id_in_your_db"}  # optional 
    posthog_privacy_mode=False # optional
)
```