---
title: LLM analytics dashboard (beta)
availability:
    free: full
    selfServe: full
    enterprise: full
---

The [LLM analytics dashboard](https://us.posthog.com/llm-analytics) provides an overview of your LLM usage and performance. It includes insights on:

- Users
- Traces
- Costs
- Generations
- Latency

<ProductScreenshot
    imageLight="https://res.cloudinary.com/dmukukwp6/image/upload/Clean_Shot_2025_01_15_at_08_31_29_4e1702243d.png"
    imageDark="https://res.cloudinary.com/dmukukwp6/image/upload/Clean_Shot_2025_01_15_at_08_31_11_66aa4e13b7.png"
    alt="LLM analytics dashboard"
    classes="rounded"
/>

It can be filtered like any dashboard in PostHog, including by event, person, and group properties. Our observability SDKs autocapture especially useful properties like provider, tokens, cost, model, and more.

This dashboard is a great starting point for understanding your LLM usage and performance. You can use it to answer questions like:

- Are users using our LLM-powered features?
- What are my LLM costs by customer, model, and in total?
- Are generations erroring?
- How many of my users are interacting with my LLM features?
- Are there generation latency spikes?

To dive into specific generation events, click on the [generations](https://us.posthog.com/llm-analytics/generations) or [traces](https://us.posthog.com/llm-analytics/traces) tabs to get a list of each captured by PostHog.

## How are costs calculated?

PostHog calculates cost based on the number of input (prompt) and output (completion) tokens generated by specific AI models. We use OpenRouter's API to fetch the prices for each model whenever possible and fall back to manually setting prices for models that OpenRouter doesn't support. 

For cached LLM response, we apply discounts to reflect the reduced costs. For example, cached tokens are charged at 50% of the normal cost for OpenAI models.

You can find the code for this on [GitHub](https://github.com/PostHog/posthog/tree/master/plugin-server/src/ingestion/ai-costs).