---
title: Billing
---


And because we charge the same as regular PostHog events, it's roughly **10x cheaper** than other LLM observability tools.

more content tk...

## How are costs calculated?

PostHog calculates cost based on the number of input (prompt) and output (completion) tokens generated by specific AI models. We use OpenRouter's API to fetch the prices for each model whenever possible and fall back to manually setting prices for models that OpenRouter doesn't support. 

For cached LLM response, we apply discounts to reflect the reduced costs. For example, cached tokens are charged at 50% of the normal cost for OpenAI models.

You can find the code for this on [GitHub](https://github.com/PostHog/posthog/tree/master/plugin-server/src/ingestion/ai-costs).