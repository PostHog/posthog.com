---
title: LLM observability installation (beta)
availability:
    free: full
    selfServe: full
    enterprise: full
---

> ðŸš§ **Note:** LLM observability is currently considered in `beta`. To access it, enable the [feature preview](https://us.posthog.com#panel=feature-previews) in your PostHog account.
>
> We are keen to gather as much feedback as possible so if you try this out please let us know. You can email [tim@posthog.com](mailto:tim@posthog.com), send feedback via the [in-app support panel](https://us.posthog.com#panel=support%3Afeedback%3Aexperiments%3Alow), or use one of our other [support options](/docs/support-options).

LLM observability enables you to capture and analyze LLM usage and performance data. Specifically, it captures:

- Input and output content and tokens
- Latency
- Model
- Traces
- Cost of each generation

All of this data is captured into PostHog to be used in insights, dashboards, alerts, and more. LLM events are charged at the same rate as other PostHog events, and so in practice are roughly **an order of magnitude cheaper** than most other LLM providers.

## Observability installation

Setting up observability starts with installing the PostHog SDK for your language.

<MultiLanguage>

```bash file=Python
pip install posthog
```

```bash file=TypeScript
npm install @posthog/ai posthog-node
```

</MultiLanguage>

The rest of the setup depends on the LLM platform you're using. These SDKs _do not_ proxy your calls, they only fire off an async call to PostHog in the background to send the data.

import Tab from "components/Tab"
import OpenAIInstall from "./_snippets/openai.mdx"
import AnthropicInstall from "./_snippets/anthropic.mdx"
import LangChainInstall from "./_snippets/langchain.mdx"
import VercelAISDKInstall from "./_snippets/vercelai.mdx"

<!-- prettier-ignore -->
<Tab.Group tabs={['OpenAI', 'Anthropic', 'Langchain', 'Vercel AI SDK']}>
    <Tab.List>
        <Tab>OpenAI</Tab>
        <Tab>Anthropic</Tab>
        <Tab>LangChain</Tab>
        <Tab>Vercel AI SDK</Tab>
    </Tab.List>
    <Tab.Panels>
        <Tab.Panel>
            <OpenAIInstall />
        </Tab.Panel>
        <Tab.Panel>
            <AnthropicInstall />
        </Tab.Panel>
        <Tab.Panel>
            <LangChainInstall />
        </Tab.Panel>
        <Tab.Panel>
            <VercelAISDKInstall />
        </Tab.Panel>
    </Tab.Panels>
</Tab.Group>

## Privacy mode

To avoid storing potentially sensitive prompt and completion data, you can enable privacy mode. This excludes the `$ai_input` and `$ai_output_choices` properties from being captured.

This can be done either by setting the `privacy_mode` config option in the SDK like this:

<MultiLanguage>

```python
import posthog

posthog.project_api_key = "<ph_project_api_key>"
posthog.host = "<ph_client_api_host>"
posthog.privacy_mode = True
```

```ts
const phClient = new PostHog(
  '<ph_project_api_key>',
  { 
    host: '<ph_client_api_host>',
    privacyMode: true
  }
)
```

</MultiLanguage>

It can also be on at the request level by including setting the `privacy_mode` parameter to `True` in the request. The exact setup depends on the LLM platform you're using:

<MultiLanguage>

```python file=OpenAI.py
client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[...],
    posthog_privacy_mode=True
)
```

```typescript file=OpenAI.ts
const completion = await openai.chat.completions.create({
    model: "gpt-3.5-turbo",
    messages: [...],
    posthogPrivacyMode: true
});
```

```python file=Anthropic
response = client.messages.create(
    model="claude-3-opus-20240229",
    messages=[...],
    posthog_privacy_mode=True
)
```

```python file=LangChain
callback_handler = PosthogCallbackHandler(
    client,
    privacy_mode=True
)
```

</MultiLanguage>