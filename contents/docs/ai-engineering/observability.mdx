---
title: LLM observability installation (beta)
availability:
    free: full
    selfServe: full
    enterprise: full
---

> ðŸš§ **Note:** LLM observability is currently considered in `beta`. To access it, enable the [feature preview](https://us.posthog.com#panel=feature-previews) in your PostHog account.
>
> We are keen to gather as much feedback as possible so if you try this out please let us know. You can email [tim@posthog.com](mailto:tim@posthog.com), send feedback via the [in-app support panel](https://us.posthog.com#panel=support%3Afeedback%3Aexperiments%3Alow), or use one of our other [support options](/docs/support-options).

LLM observability enables you to capture and analyze LLM usage and performance data. Specifically, it captures:

- Input and output content and tokens
- Latency
- Model
- Traces
- Cost of each generation

All of this data is captured into PostHog to be used in insights, dashboards, alerts, and more. LLM events are charged at the same rate as other PostHog events, and so in practice are roughly **an order of magnitude cheaper** than most other LLM providers.

## Observability installation

Setting up observability starts with installing the PostHog SDK for your language.

<MultiLanguage>

```bash file=Python
pip install posthog
```

```bash file=TypeScript
npm install @posthog/ai posthog-node
```

> **Note:** You can use LLM observability with any of our SDK's however you will need to capture the data manually via the capture method. See schema of event [here](#manual-capture).

</MultiLanguage>

The rest of the setup depends on the LLM platform you're using. These SDKs _do not_ proxy your calls, they only fire off an async call to PostHog in the background to send the data.

import Tab from "components/Tab"
import OpenAIInstall from "./_snippets/openai.mdx"
import AnthropicInstall from "./_snippets/anthropic.mdx"
import LangChainInstall from "./_snippets/langchain.mdx"
import VercelAISDKInstall from "./_snippets/vercelai.mdx"

{/* <!-- prettier-ignore --> */}
<Tab.Group tabs={['OpenAI', 'Anthropic', 'Langchain', 'Vercel AI SDK']}>
    <Tab.List>
        <Tab>OpenAI</Tab>
        <Tab>Anthropic</Tab>
        <Tab>LangChain</Tab>
        <Tab>Vercel AI SDK</Tab>
    </Tab.List>
    <Tab.Panels>
        <Tab.Panel>
            <OpenAIInstall />
        </Tab.Panel>
        <Tab.Panel>
            <AnthropicInstall />
        </Tab.Panel>
        <Tab.Panel>
            <LangChainInstall />
        </Tab.Panel>
        <Tab.Panel>
            <VercelAISDKInstall />
        </Tab.Panel>
    </Tab.Panels>
</Tab.Group>

## Privacy mode

To avoid storing potentially sensitive prompt and completion data, you can enable privacy mode. This excludes the `$ai_input` and `$ai_output_choices` properties from being captured.

This can be done either by setting the `privacy_mode` config option in the SDK like this:

<MultiLanguage>

```python
import posthog

posthog.project_api_key = "<ph_project_api_key>"
posthog.host = "<ph_client_api_host>"
posthog.privacy_mode = True
```

```ts
const phClient = new PostHog(
  '<ph_project_api_key>',
  { 
    host: '<ph_client_api_host>',
    privacyMode: true
  }
)
```

</MultiLanguage>

It can also be on at the request level by including setting the `privacy_mode` parameter to `True` in the request. The exact setup depends on the LLM platform you're using:

<MultiLanguage>

```python file=OpenAI.py
client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[...],
    posthog_privacy_mode=True
)
```

```typescript file=OpenAI.ts
const completion = await openai.chat.completions.create({
    model: "gpt-3.5-turbo",
    messages: [...],
    posthogPrivacyMode: true
});
```

```python file=Anthropic
response = client.messages.create(
    model="claude-3-opus-20240229",
    messages=[...],
    posthog_privacy_mode=True
)
```

```python file=LangChain
callback_handler = PosthogCallbackHandler(
    client,
    privacy_mode=True
)
```

</MultiLanguage>

## Manual capture

If you're using a different SDK, you can manually capture the data by calling the `capture` method.

{/* <!-- prettier-ignore --> */}
<Tab.Group tabs={['Generation', 'Trace', 'Span', 'Embedding']}>
    <Tab.List>
        <Tab>Generation</Tab>
        <Tab>Trace</Tab>
        <Tab>Span</Tab>
        <Tab>Embedding</Tab>
    </Tab.List>
    <Tab.Panels>
        <Tab.Panel>
            A generation is a single call to an LLM.
            ```
            Event Name: $ai_generation
            Distinct ID: User ID or Anonymous ID (optional)
            Properties:
                - $ai_trace_id # The trace ID of the request (a UUID to group ai events)
                - $ai_model # The model used (e.g. `gpt-3.5-turbo`)
                - $ai_provider # The LLM provider
                - $ai_input # List of messages
                    - e.g. `[{"role": "user", "content": "Tell me a fun fact about hedgehogs"}]`
                - $ai_input_tokens # The number of tokens in the input (often found in response.usage)
                - $ai_output_choices # List of choices
                    - e.g. `[{"role": "assistant", "content": "Hedgehogs are small mammals with spines on their back."}]`
                - $ai_output_tokens # The number of tokens in the output (often found in response.usage)
                - $ai_latency # The latency of the LLM call (tracked yourself)
                - $ai_http_status # The HTTP status code of the response
                - $ai_base_url # The base URL of the LLM provider
                - $ai_is_error # boolean to indicate if the request was an error
                - $ai_error # The error message or object 
            ```
        </Tab.Panel>
        <Tab.Panel>
            The trace event is used to store additional context about for a given traceID.
        </Tab.Panel>
        <Tab.Panel>
            A span is a single action withing your application i.e. a function call or vector database search.
        </Tab.Panel>
        <Tab.Panel>
            An embedding is a way to track the embedding of a given text.
        </Tab.Panel>
    </Tab.Panels>
</Tab.Group>

test