---
title: LLM observability installation (beta)
availability:
    free: full
    selfServe: full
    enterprise: full
---

> ðŸš§ **Note:** LLM observability is currently considered in `beta`. To access it, enable the [feature preview](https://us.posthog.com#panel=feature-previews) in your PostHog account.
>
> We are keen to gather as much feedback as possible so if you try this out please let us know. You can email [tim@posthog.com](mailto:tim@posthog.com), send feedback via the [in-app support panel](https://us.posthog.com#panel=support%3Afeedback%3Aexperiments%3Alow), or use one of our other [support options](/docs/support-options).

LLM observability enables you to capture and analyze LLM usage and performance data. Specifically, it captures:

- Input and output content and tokens
- Latency
- Model
- Traces
- Cost of each generation

All of this data is captured into PostHog to be used in insights, dashboards, alerts, and more. LLM events are charged at the same rate as other PostHog events, and so in practice are roughly **an order of magnitude cheaper** than most other LLM providers.

## Observability installation

Setting up observability starts with installing the PostHog Python SDK.

```bash
pip install posthog
```

The rest of the setup depends on the LLM platform you're using. These SDKs _do not_ proxy your calls, they only fire off an async call to PostHog in the background to send the data.

import Tab from "components/Tab"
import OpenAIInstall from "./_snippets/openai.mdx"
import LangChainInstall from "./_snippets/langchain.mdx"

<!-- prettier-ignore -->
<Tab.Group tabs={['OpenAI', 'Langchain']}>
    <Tab.List>
        <Tab>OpenAI</Tab>
        <Tab>LangChain</Tab>
    </Tab.List>
    <Tab.Panels>
        <Tab.Panel>
            <OpenAIInstall />
        </Tab.Panel>
        <Tab.Panel>
            <LangChainInstall />
        </Tab.Panel>
    </Tab.Panels>
</Tab.Group>

## Privacy mode

To avoid storing potentially sensitive prompt and completion data, you can enable privacy mode. This excludes the `$ai_input` and `$ai_output` properties from being captured.

This can be done either by setting the `privacy_mode` config option in the Python SDK like this:

```python
import posthog

posthog.project_api_key = "<ph_project_api_key>"
posthog.host = "<ph_client_api_host>"
posthog.privacy_mode = True
```

It can also be on at the request level by including setting the `privacy_mode` parameter to `True` in the request. The exact setup depends on the LLM platform you're using:

<MultiLanguage>

```python file=OpenAI
client.chat.completions.create(
    messages=[...],
    posthog_privacy_mode=True
)
```

```python file=LangChain
callback_handler = PosthogCallbackHandler(
    client,
    privacy_mode=True
)
``` 

</MultiLanguage>