---
title: Experimentation
sidebar: Docs
showTitle: true
---

<FeatureAvailability availablePlans={['standard', 'enterpriseCloud', 'startup', 'scale', 'enterprise']} />

Experimentation helps you test product changes to your app and find the optimal changes. You can test hypotheses about which product changes will lead to optimal results. You may have heard it called A/B testing before. Examples:

-   You want to test which copy or visual design has the highest conversion.
-   You want to test which flow leads to the highest number of purchases.
-   Another use case here?

> ðŸš§ **Experimentation is currently in beta** and support is limited to our [JavaScript](/docs/integrate/client/js#feature-flags) integration. If you have any feedback, please <a href="https://posthog.com/support" target="_blank">reach out</a>.

## How does it work?

With Experimentation, you start with a hypothesis on how to improve your product (e.g. changing the flow, changing copy, changing visual styles, ...) and a goal (e.g. improve conversion, improve sign ups, ...). Once you have those you can:

1. Create an Experiment which will have a control group and any number of experimentation variants (at least one).

2. Select which users will participate in the Experiment (e.g. paid users, users in a given country, or all users).

Based on your user selection and your goal, we will estimate the number of users exposed and the number of days required for your Experiment to reach significant results.

You can tweak your settings to speed up the experiment or adjust user exposure (e.g. you may not want certain users exposed to your experiment).

The actual experiment and what your user sees is controlled in your own code (see below on [Creating an experiment](#creating-an-experiment)). You ask PostHog which group does this user belong to and then you show any relevant changes in your app or website. Because this is done in your code you can test any number of things, from changing copy in a button, to taking the user to a different page, or showing them a completely different experience.

Once your experiment is running, we'll automatically track your main conversion and let you know which variant leads to the best results. While the experiment is running, we'll also tell you the probability that any given variant is the winning variant.

To ensure your experiment does not introduce any regressions, you can also track secondary metrics. This is particularly useful if you're optimizing a metric that has a natural trade-off with another metric (e.g. you can maximize conversion by reducing your price but then your total revenue may be reduced).

Once your experiment is terminated, you can easily archive and share your results with other team members.

### Results significance

In order for your results and conclusions to be valid, any experiment must have a significant exposure. For instance, if you test a product change and only one user sees the change, you can't extrapolate from that single user that the change will be beneficial/detrimental for your entire user base. This is true for any experiment that uses the scientific method (e.g. this is also done when testing new drugs or vaccines).

PostHog computes this significance for you automatically - we will let you know if your experiment has reached significant results or not. Once your experiment reaches significant results, it's safe to use those results to reach a conclusion and terminate the experiment. You can read more about how we do this in our 'Advanced' section below

### Experimentation vs. Feature Flags

Experimentation and Feature Flags are different features because they serve different purposes. **Experimentation** is used to test changes to your product where you want to maximize a specific metric (eg. testing a new flow to maximize conversion rate). **Feature Flags** are used for phased roll-outs (e.g. a new experimental feature you want to make sure works correctly) or as a way to control feature access. Read our [Feature Flags user guide](/docs/user-guides/feature-flags) for more information on this.

Experimentation uses Feature Flags under the hood to handle user allocation to an experiment. You'll notice this when checking if a user is assigned to an experiment.

## Using Experimentation

### Creating an experiment

TODO: Guide on how to create an experiment here.

### Running an experiment

TODO: Guide on how to run an experiment here.

### Terminating an experiment

You decide when to terminate an experiment. A banner in your experiment results page will let you know if it's safe to terminate an experiment or not (regardless of the original estimated running time). If you terminate the experiment before you see this banner, results will not be significant (see Advanced for more details on this) and any conclusions could be completely wrong.

Once you have decided to terminate an experiment there are a few things to do:

-   [ ] Click on "Terminate the experiment" on the experiment page. This will ensure final results are kept.
-   [ ] We recommend you roll-out the winning variant to all your users. TODO: Explain here how to do this with PostHog.
-   [ ] Share results with relevant members of your team.
-   [ ] After the winning variant is rolled-out, it's good practice to remove the other variants from your code, and make the winning variant part of your core code (i.e. stop checking for the respective Feature Flag).
-   [ ] Document conclusions and findings in your PostHog experiment. This will help preserve context for team members who in the future need to understand what happened.
-   [ ] Archive the experiment. This will terminate the Feature Flag and ensure your code no longer depends on the flag.

## Advanced: What's under the hood?

TODO: Advanced math details on how experimentation works, how we compute significance, safe to terminate state, etc.

### How we ensure distribution of people

For every experiment, we leverage PostHog's multivariate feature flags. There's one control group, and up to 3 test groups. Based on their distinctID, each user is randomly distributed into one of these groups. This is stable, so the same users, even when they revisit your page, stay in the same group.

We do this by creating a hash out of the feature flag key and the distinctID. It's worth noting that when you have low data (<1,000 users per variant), the difference in variant exposure can be up to 20%. This means, a test variant could have 800 people only, when control has 1,000.

All our calculations take this exposure into account.

### Recommendations for sample size and running time

When you're creating an experiment, we show you recommended running times and sample sizes based on parameters you've chosen.

For trend experiments, we use Lehr's equation [as explained here](http://www.columbia.edu/~cjd11/charles_dimaggio/DIRE/styled-4/code-12/#poisson-distributed-or-count-data) to determine sample sizes.

```
exposure = 4 / (sqrt(lambda1) - sqrt(lambda2))^2
```

where lambda1 is the baseline count data we've seen for the past two weeks,
and lambda2 is `baseline count + mde*(baseline count)`.

`mde` is the minimum acceptable improvement you choose in the UI.

For funnel experiments, we use the general [Sample Size Determination](https://en.wikipedia.org/wiki/Sample_size_determination) formula, with 80% power and 5% significance. The formula then becomes:

```
sample size per variant = 16 * conversion_rate * (1 - conversion_rate) / (mde)^2
```

where `mde` is again the minimum detectable effect chosen in the UI.

We give these values as an estimate for how long to run the experiment. It's possible to end experiments before you reach the end, if you see an outsized effect.

### Bayesian A/B Testing

We follow a mostly bayesian approach to A/B testing. While running any experiment, we calculate two parameters: (1) Probability of each variant being the best, and (2) whether the results are significant or not.

Below are calculations for each kind of experiment.

### Trend Experiment Calculations

Trend experiments capture count data. For example, if you want to measure the change in total count of clicks, you'd use this kind of experiment.

We use monte carlo simulations to determine the probability of each variant being the best. A variant 

### Funnel Experiment Calculations
