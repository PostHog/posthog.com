---
title: Deploying to Google Cloud Platform
sidebarTitle: Google Cloud Platform
sidebar: Docs
showTitle: true
tags:
    - gcp
---

import ClusterRequirementsSnippet from './snippets/cluster-requirements'
import InstallingSnippet from './snippets/installing'
import UpgradingSnippet from './snippets/upgrading'
import UninstallingSnippet from './snippets/uninstalling'
import SetupDNS from './snippets/setup-dns'
import NextSteps from './snippets/next-steps'

First, we need to set up a Kubernetes cluster (see the official GCP [documentation](https://cloud.google.com/kubernetes-engine/) for more info).

<ClusterRequirementsSnippet />

> Note: in order to reduce the overhead of managing stateful services like PostgreSQL, Kafka, Redis and ClickHouse by yourself, we suggest you to run them _outside_ Kubernetes and offload their provisioning, building and maintenance operations:
>
> -   for PostgreSQL, take a look at [Google Cloud SQL for PostgreSQL](https://cloud.google.com/sql/docs/postgres)
> -   for Apache Kafka, take a look at [Confluent Cloud](https://www.confluent.io/confluent-cloud/)
> -   for Redis, take a look at [Google Cloud Memorystore](https://cloud.google.com/memorystore) and [Redis Enterprise Cloud](https://redis.com/redis-enterprise-cloud/overview/)
> -   for ClickHouse, take a look at [Altinity Cloud](https://altinity.com/cloud-database/)

## Chart configuration

Here's the minimal required `values.yaml` that we'll be using later. You can find an overview of the parameters that can be configured during installation under [configuration](/docs/self-host/deploy/configuration).

```yaml
cloud: 'gcp'
ingress:
    hostname: <your-hostname>
```

## Installing the chart

<InstallingSnippet />

## Set up a static IP

**Important** - This must be a `Global` IP address. GKE will not be able to find a `Region` IP address and assign it to the ingress controller's LB.

### In GCP web console

1. Open the Google Cloud Console
1. Go to VPC Networks > [External IP addresses](https://console.cloud.google.com/networking/addresses/list)
1. Add a new **global** static IP with the name `posthog`

### From gcloud CLI

1. `gcloud compute addresses create posthog --global`

## Setting up DNS

<SetupDNS />

## Next steps

<NextSteps />

## Troubleshooting

### I cannot connect to my PostHog instance after creation

If DNS has been updated properly, check whether the SSL certificate was created successfully.

This can be done via the following command:

```shell
gcloud beta --project yourproject compute ssl-certificates list
```

If running the command shows the SSL cert as `PROVISIONING`, that means that the certificate is still being created. [Read more on how to troubleshoot Google SSL certificates here](https://cloud.google.com/load-balancing/docs/ssl-certificates/troubleshooting).

As a troubleshooting tool, you can allow HTTP access by setting `ingress.gcp.forceHttps` and `web.secureCookies` both to false, but we recommend always accessing PostHog via https.

### Upgrading the chart

<UpgradingSnippet />

### Uninstalling the chart

<UninstallingSnippet />

## Clickhouse Configuration

By default clickhouse is provisioned with standard gcp persistent disk. If you want to specify your own persistent volume claim or switch to a different type of disk
you can specify the volume claim within `values.yaml`.

### To manually provision a disk

Using the gcloud cli tool for provisioning a disk:

```
gcloud compute disks create pvc-clickhouse --type=pd-ssd --size=2048GB --zone=us-central1-c
```

### Create the claim

In order to provide the disk to the clickhouse deployment you must first create a persistent volume and claim within the posthog namespace.

```
# This creates a volume claim using the same name specified within the clickhouse values file
apiVersion: v1
kind: PersistentVolume
metadata:
  name: clickhouse-volume
spec:
  persistentVolumeReclaimPolicy: Retain
  storageClassName: ""
  capacity:
    storage: 2048Gi
  accessModes:
    - ReadWriteOnce
  gcePersistentDisk:
    pdName: pvc-clickhouse
    fsType: ext4
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: clickhouse-pvc
spec:
  # It's necessary to specify "" as the storageClassName
  # so that the default storage class won't be used, see
  # https://kubernetes.io/docs/concepts/storage/persistent-volumes/#class-1
  storageClassName: ""
  volumeName: clickhouse-volume
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2048Gi
```

### Provide the claim to the helm chart

Add the following to your `values.yaml` & run `helm install` or `upgrade`:

```
clickhouse:
  # -- Optional: Used to manually specify a persistent volume claim. When specified the cloud specific storage class will not be provisioned
  persistentVolumeClaim: "clickhouse-pvc"
```
