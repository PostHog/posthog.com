---
title: Disaster recovery considerations
sidebarTitle: Disaster recovery
sidebar: Docs
showTitle: true
---

It's important to make sure you understand how to recovery from scenarios before
they happen.

## Data considerations

Data is stored in PostgreSQL, ClickHouse and Kafka. At a minimum you should have
backups of PostgreSQL and ClickHouse, these are crucial to PostHog. Kafka stores
the incoming analytics events which are then persisted into ClickHouse.

## PostgreSQL

PostgreSQL stores your PostHog instance configuration and authentication/authorization.

We recommend that you use a provider that supports backups. For
example, AWS RDS will [handle backups and restores for you](https://aws.amazon.com/rds/features/backup/).
Other cloud providers offer similar services.

## ClickHouse

For ClickHouse, see our [ClickHouse backup runbook](/docs/runbook/services/clickhouse/backup).

ClickHouse consumes from Kafka using a static consumer group. Make sure that you
only ever have one cluster consuming at a time. When bringing up a cluster from backup
it is a good idea to ensure that the consumer cursors are reset such that the cluster
consumes any events in Kafka that may be missing from your ClickHouse the backup.

## Kafka

Kafka is the first place events are persisted before being consumed by other services,
and ultimately ending up in ClickHouse. Solutions exist for streaming this data to
long term storage. For example, with AWS you can use either the
[Lenses.io S3 Kafka Connector](https://aws.amazon.com/blogs/big-data/back-up-and-restore-kafka-topic-data-using-amazon-msk-connect/)
or [AWS Kinesis Firehose](https://aws.amazon.com/blogs/architecture/amazon-msk-backup-for-archival-replay-or-analytics/).

If PostHog is consuming events to ClickHouse in good time, you should not lose much
in case of a Kafka failure, so it may be a risk that you want to accept.
