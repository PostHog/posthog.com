---
title: Funnels statistics
---

Funnel experiments use a Beta model to evaluate the **win probabilities** and **credible intervals** for an experiment. [Read the statistics primer](/docs/experiments/statistics-primer) for an introduction to Bayesian statistics if you haven't already.

## What the heck is a Beta model?

The Beta model is great for analyzing proportions (0% to 100%) - like whether someone completes a signup flow or not. Let's break it down with a simple example:

Imagine you run a pizza shop and want to know if customers say "yes" to adding pineapple. Some customers will say yes, others will say no. This kind of binary outcome is what's called a **Beta distribution**.

The Beta distribution helps us model our uncertainty about the true "yes" rate:
- When we have very little data, the Beta distribution is wide, saying "hey, the true rate could be anywhere between 0% and 100%".
- As we collect more data, the Beta distribution gets narrower, saying "we're getting more confident about what the true rate is".

For example, if:
- Day 1: 2 out of 4 customers say yes (50%), we're still very uncertain.
- Day 30: 150 out of 300 customers say yes (50%), we're much more confident it's around 50%.

So when we say we're using a Beta model for experiments, we're:
1. Using the Beta distribution to express our uncertainty about the true rate.
2. Updating our understanding as we collect more data.
3. Getting more confident in our estimates over time.

Our Beta model uses a minimally informative prior of `ALPHA_PRIOR = 1` and `BETA_PRIOR = 1`.

## Win probabilities

The **win probability** tells you how likely it is that a given variant has the highest conversion rate compared to all other variants in the experiment. It helps you determine whether the experiment shows a **statistically significant** real effect vs. simply random chance.

Let's say you're testing a new signup flow and have these results:
- Control: 100 signups from 1000 visitors (10% conversion)
- Test: 150 signups from 1000 visitors (15% conversion)

To calculate the win probabilities for the experiment, our methodology:

1. Models each variant's conversion rate using a Beta distribution:
	- Control: Beta(100 + ALPHA_PRIOR, 900 + BETA_PRIOR)
	- Test: Beta(150 + ALPHA_PRIOR, 850 + BETA_PRIOR)

2. Takes 10,000 random samples from each distribution.

3. Checks which variant had the higher conversion rate for each sample.

4. Calculates the final win probabilities:
	- Control wins in 40 out of 10,000 samples = 0.4% probability
	- Test wins in 9,960 out of 10,000 samples = 99.6% probability

These results tell us we can be 99.6% confident that the test variant performs better than the control.

## Credible intervals

A **credible interval** tells you the range where the true conversion rate lies with 95% probability. Unlike traditional confidence intervals, credible intervals give you a direct probability statement about the conversion rate.

For example, if you have these results:
- Control: 100 signups from 1000 visitors (10% conversion)
- Test: 150 signups from 1000 visitors (15% conversion)

To calculate the credible intervals for the experiment, our methodology will:

1. Create a Beta distribution for each variant:
	- Control: Beta(100 + ALPHA_PRIOR, 900 + BETA_PRIOR)
	- Test: Beta(150 + ALPHA_PRIOR, 850 + BETA_PRIOR)

2. Find the 2.5th and 97.5% percentiles of each distribution:
	- Control: [8.3%, 12%] = "You can be 95% confident the true conversion rate is between 8.3% and 12.0%"
	- Test: [12.9%, 17.3%] = "You can be 95% confident the true conversion rate is between 12.9% and 17.3%"

Since these intervals don't overlap, you can be quite confident that the test variant performs better than the control. The intervals will become narrower as you collect more data, reflecting your increasing certainty about the true conversion rates.