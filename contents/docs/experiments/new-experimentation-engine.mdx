---
title: New experimentation engine
---

We're building a new experimentation engine to support more advanced features and improve the accuracy of experiment results. It's ready to try out, and we’d love your feedback if you give it a spin!

Here are some of the key features included in the new engine:

* A single, consistent exposure criterion for the entire experiment
* Support for outlier handling (Winsorization)
* Exclusion of users exposed to multiple variants
* Improved support for conversion windows and time-based metrics (formerly called trend metrics)
* A better running time calculator that uses your historical data to estimate experiment duration

The user interface has also been updated. You’ll now see a chart showing experiment exposures, as well as the total number of users in each variant. There's also a count of users who were exposed to more than one variant.

The UI is still a work in progress, so please share your thoughts—your feedback helps us improve!

Read on for a deeper dive into the new features.

---

### Running time calculator

The new running time calculator provides a more accurate estimate of how long your experiment will need to run. It walks you through a few steps:

* **Traffic estimation**: Choose an event to estimate incoming traffic. For example, a pageview event with a specific path or URL filter.
* **Select metric**: Pick one of your added metrics—ideally, your primary success metric—for estimating running time.
* **Minimum detectable effect**: Set the minimum relative change you'd like to detect. Smaller changes require more data and longer run times. A common range is between 5% and 30%, depending on the impact you're trying to measure and your daily user volume.

<ProductScreenshot
  imageLight = "https://res.cloudinary.com/dmukukwp6/image/upload/running_time_calc_light_2_02a23ce16b.png"
  imageDark = "https://res.cloudinary.com/dmukukwp6/image/upload/running_time_calc_dark_2_228e0dca78.png"
  classes="rounded"
  alt="Screenshot of the running time calculator"
/>

---

### Experiment exposure

This defines which users should be included in the experiment analysis. By default, we use the `$feature_flag_called` event, which typically indicates that a user was exposed to the experiment (i.e., saw the part of your app or site where the experiment is running). Only users who meet this criterion are included in the analysis.

At the top of the experiment view, you'll see the total number of unique users in each variant. Users who saw more than one variant are grouped under `$multiple` and excluded from the analysis.

Metric events are only counted if they occur *after* a user's first exposure.

<ProductScreenshot
  imageLight = "https://res.cloudinary.com/dmukukwp6/image/upload/exposure_light_ec214e48ed.png"
  imageDark = "https://res.cloudinary.com/dmukukwp6/image/upload/exposure_dark_32df91d9a6.png"
  classes="rounded"
  alt="Screenshot of outlier handling"
/>

---

### Configuring metrics

#### Funnel

This works similarly to funnels in Product Analytics. Use it to measure conversion rates. Currently, we only show results for the final step of the funnel, but we’re working on supporting breakdowns by step soon.

When you add a funnel metric, we insert a placeholder for the experiment exposure in the preview. That’s because using the actual exposure criteria as the first step would typically result in no data being shown. During evaluation, your chosen exposure event will serve as the actual first step. This means you can add funnels with just one step—exposure is implicitly treated as step one.

#### Mean

Use this to track metrics like total event count or summed values (e.g., revenue). Here’s how it works:

1. We aggregate the chosen value for each user using your selected method (count, sum, or average).
2. We then compute the *mean* of those values for each variant.
3. This mean is used to determine the statistical difference between variants.

For example:
- If you choose “sum of revenue,” we calculate total revenue per user, then take the mean across users.
- If you select “average revenue,” we first compute each user’s average (total revenue / number of events), then average those values across users. (NB: average is not yet supported, but coming soon!)

#### Ratio

Not yet supported—but it's on the roadmap!


### Outlier handling

Limit the impact of extreme values by capping metric data at specified percentile thresholds, using configurable lower and upper bounds. This technique—commonly known as Winsorization—is especially useful for metrics with highly skewed distributions, such as revenue or total event counts, where a small subset of users can produce unusually high values that distort overall results.

<ProductScreenshot
  imageLight = "https://res.cloudinary.com/dmukukwp6/image/upload/outlier_handling_light_8139933041.png"
  imageDark = "https://res.cloudinary.com/dmukukwp6/image/upload/outlier_handling_dark_c7c0d775d6.png"
  classes="rounded"
  alt="Screenshot of outlier handling"
/>


### Conversion windows

Restrict metric events to those that occur within a defined time window following a user’s initial exposure to the experiment. This ensures that only relevant, post-exposure behavior is included in the analysis.

<ProductScreenshot
  imageLight = "https://res.cloudinary.com/dmukukwp6/image/upload/conversion_window_limit_light_25b34f15b9.png"
  imageDark = "https://res.cloudinary.com/dmukukwp6/image/upload/conversion_window_limit_dark_c07beddef8.png"
  classes="rounded"
  alt="Screenshot of conversion window configuration"
/>
