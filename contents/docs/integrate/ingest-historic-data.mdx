---
title: Ingest Historical Data
sidebar: Docs
showTitle: true
---

import { Tabs } from 'antd'
export const TabPane = Tabs.TabPane

Historical data ingestion (or importing data) is the process of transporting data from external sources into PostHog so you can benefit from PostHog product analytics. It may be that you have historical data that you want to analyze along with new live data or that you have a requirement to periodically import data from third-party sources to augment your live data.

Whatever the reason for the historical data ingestion, this guide covers what to consider during that process.

The three main factors to consider are:

- [Data ingestion process](#data-ingestion-process): how to get the event data from the third-party source into PostHog
- [Importing events](#importing-events): Sending the events captured in the third-party data source into PostHog as custom events
- [User identification](#user-identification): How to identify users within PostHog and ties those users back to the user within original data source

## Data ingestion process

PostHog doesn't presently offer any specific tools to import or migrate historical data from third-party data sources. However, since the third-party data source is highly likely to offer an API you can use the power of software to import the data from one or more sources to PostHog.

The following factors are important in the export and then import process:

- The volume of data
- API rate limits of both the data source and PostHog
- Ensure that only the data required for events is exported
- Handle error scenarios allowing the process to resume form the last successful point

With the above factors in mind it's recommended that you break the process up into steps such as the following:

1. Sequencially export selective data from the data source that represent key events keeping track of where you are in the sequence so that you can restart the process from the last successful point if any problems occur
2. Store the selected exported data to a new data storage for faster access in future steps
3. Transform the data to the format you will use with PostHog and again save to a storage mechamism for faster access in later steps.

    The data format should be:
    ```json
    {
        "event": "[event name]",
        "properties": {
            "distinct_id": "[your users' distinct id]",
            "key1": "value1",
            "key2": "value2"
        },
        "timestamp": "[optional timestamp in ISO 8601 format]"
    }
    ```

    At this stage it's also important to consider the following:
    
    1. Use the same `event` name that you're going to use with your [live data ingestion](/docs/integrate/ingest-live-data) so the historical and live events are seen as the same type within PostHog
    1. Use the same unique identifier within the `distinct_id` field as you are within your [live data ingestion](/docs/integrate/ingest-live-data) so historical events and live events are associated with the same user
    1. Convert the old event property names to the new event property names you are using within the events in your [live data ingestion](/docs/integrate/ingest-live-data)
    1. Ensure that the `timestamp` is a converted version of the original timestamp is ISO 8601 format so that PostHog correctly identifies when the original event occurred
    1. You may want to set an additional property within `properties` that identifies the original event within the data source

4. Sequencially import the events into PostHog keeping track of the last successfuly imported event so that you can restart the process from the last successful point if any problems occur

## Importing events

Once you are ready to import the data into PostHog you can use one of the following:

- a PostHog [server libraries](/docs/integrate/overview#server-libraries)
- the [PostHog API](/docs/api/overview)

As mentioned above, the data should be in the following format:

```json
{
    "event": "[event name]",
    "properties": {
        "distinct_id": "[your users' distinct id]",
        "key1": "value1",
        "key2": "value2"
    },
    "timestamp": "[optional timestamp in ISO 8601 format]"
}
```

import NodeCapture from './server/node/snippets/capture.mdx'
import PHPCapture from './server/php/snippets/capture.mdx'
import APICapture from './server/api/snippets/capture.mdx'

> The server libraries handle batching capture requests. If you decide to use the API directly you will need to manage this yourself.

<Tabs defaultActiveKey="1" type="card" size="large" style={{marginBottom: "20px"}}>
    <TabPane tab="Node.js" key="1">
        <NodeCapture />
    </TabPane>
    <TabPane tab="PHP" key="2">
        <PHPCapture />
    </TabPane>
    <TabPane tab="API" key="3">
        <APICapture />
    </TabPane>
</Tabs>

## User identification

As discussed within the [data ingestion process section](#data-ingestion-process), a unique user identifier should be set for each event. In addition to setting the user with each event you can enrich information about that user by adding more properties:

import NodeIdentify from './server/node/snippets/identify.mdx'
import PHPIdentify from './server/php/snippets/identify.mdx'

<Tabs defaultActiveKey="1" type="card" size="large" style={{marginBottom: "20px"}}>
    <TabPane tab="Node.js" key="1">
        <NodeIdentify />
    </TabPane>
    <TabPane tab="PHP" key="2">
        <PHPIdentify />
    </TabPane>
</Tabs>