---
title: LLM Analytics
sidebar: Handbook
showTitle: true
hideAnchor: false
template: team
---

### Who we're building for
**Product Engineers** and **Full Stack Developers** who are building:
- AI-native products (agents, assistants, copilots, specialized hardware) 
- AI-adjacent products (LLMs integrated into existing products)

**LLM analytics is a good fit if:**
- They need to monitor traces, spans, token costs, latency, and analyze usage of AI features
- They're using trace summaries to debug and evals to make product decisions
- They care about questions like: “how does interacting with LLM features correlate with retention, usage, or revenue?”
- They're already PostHog users (or should be) using [product analytics](/product-analytics) and [session replay](/session-replay) to combine qualitative context with quantitative data
- They want to start getting value right away, without needing extensive setup and configuration

**Who else might want to use LLM analytics at their org:**
- **Application Ops / SRE** to monitor production AI systems for errors, prompt injection, jailbreaks, or other security issues
- **Product Managers** to understand user sentiment, usage and make decisions about their AI roadmap
- **Customer Success / Support Teams** to improve documentation or investigate user issues

### Who we're NOT building for (right now)
**AI Researchers** and **Machine Learning (ML) Engineers** doing:
- Deep foundation model work
- Complex benchmarking and evaluation
- Advanced experimentation requiring specialized tooling

These folks are running CI/CD pipelines and building full QA automation frameworks with performance benchmarks. 
If they try us and churn, that's fine. We haven't built the tools they need (yet).
