**Data Team's Mission at PostHog**

Data Team's mission is to provide a storage and query engine that meets these requirements:
- Continue to meet the needs of the product today now and in the future
- Maintain and optimize our current ClickHouse deployment
- Elastically scale our capacity with little effort
- Support multiple query quality of service (QOS) guarantees (Real-time, Batch, etc.)
- Data is stored once and queryable from the appropriate tool
- Queries are optimized for cost and performance
- Tunable execution performance to allow trade-offs between cost and performance
- Storage is durable

In service of this mission, our goals are:

**Goals for Q2 2025:**

- Query Observability and Performance Improvements
    - we want to know where the query is coming from and why. Is it per team or posthog wide, is it by our system or by customer?
    - system.query_log table, log_comment column should have proper tags: tag_queries(team_id, issuer, product)
    - Add additional metadata tags to better identify and categorize API query sources

- Make ClickHouse ops easy
    - Complete cluster management through Infrastructure as Code (IaC)
    - Optimize speed of critical operational procedures incl. automation for common operations (e.g. ZK)
    - Implement ClickHouseKeeping automation via Dagster (cleanup jobs, backups, improve replication performance)
    - Better runbooks for everyone
    - Build automation for handling ad-hoc deletion requests and disk provisioning  
    - Storage infrastructure improvements (s3-backed events, get rid of io2)

- Use Altinity Antalya in production for events
    - upgrade clusters to Antalya 
    - get DW queries running through Antalya Swarm
    - Backfill all events onto Iceberg in S3
