---
date: 2026-01-22
title: "F*$k youuuu standup bot!"
author:
  - andy-maguire
rootPage: /blog
sidebar: Blog
showTitle: true
hideAnchor: true
featuredImage: >-
  https://res.cloudinary.com/dmukukwp6/image/upload/posthog.com/contents/images/blog/posthog-engineering-blog.png
featuredImageType: full
category: Engineering
tags:
  - Engineering
  - LLM analytics
---

import { CalloutBox } from 'components/Docs/CalloutBox'

{/*TODO: Replace featured image with something fun - hedgehog vs robot?*/}

![The standup bot, taunting me](https://res.cloudinary.com/dmukukwp6/image/upload/q_auto,f_auto/standup_bot_taunt_b24351e046.png)

There it is. Starign at me. Sitting there. Goading me.

The day is nearly done. I've reviewed PRs, fixed bugs, hopped on quick calls, commanded my agents and smashed that accept button all day!

And now I'm expected to fill in some stupid Slack thread with my standup updates?

I'm too good for this. I don't need this corporate nonsense. I'm gonna stick it to the man. I'm not gonna play their silly little games.

I'm going to beat the system.

OK standup bot, you want a standup update? No problem my friend. You just wait...

## The breaking point

Every time I manually copy-paste into our standup thread, a little part of me dies inside.

We just passed the Turing test and here I am copy-pasting GitHub issue URLs into a Slack thread like it's 2010. I'm pulling up tabs, hunting for PR links, navigating all over GitHub, trying to remember what I (my agents) actually did today, and formatting it all nicely in slack.

After a few months of this, I snapped.

I work on LLM analytics at PostHog. I spend my days helping people understand what their AI agents are doing. And yet here I am, manually summarizing my GitHub activity like some kind of caveman.

The irony was too much. I decided to build my own agent to beat the standup bot at its own game.

It helps too that this was also the perfect excuse to learn about and play around a little with the OpenAI Agents SDK.

## The plan

The idea was simple:

1. Build an AI agent that reads my GitHub activity and previous recent standups for context
2. Have it draft a sensible standup summary in the style I want
3. Chat with it to refine the draft
4. Post directly to Slack

And because I'm a shameless dogfooder, this was the perfect excuse to:

- Learn the [OpenAI Agents SDK](https://github.com/openai/openai-agents-python)
- Build instrumentation for it in [posthog-python](https://github.com/PostHog/posthog-python)
- Create something I can actually use every day
- Write this very blog post
- But most importantly, use AI even if it actually sometimes takes me longer each day than it did before.

Let me show you how it all works.

<CalloutBox icon="IconBolt" title="ðŸ”¥ Hot take" type="action">

**Agents are the new scripts.** We used to write bash scripts and cron jobs to automate tedious tasks. Now we write agents. Scripts follow rigid rules; agents are more flexible and often thats worth it.

</CalloutBox>

## The architecture: one agent to rule them all

I started by sketching out a multi-agent system - a coordinator, a data gatherer, a summarizer - but quickly realized: why complicate things? One agent with the right tools can handle the entire workflow. Gather data, summarize it, refine it, post it. No handoffs, no routing logic, no coordination overhead.

The agent has various tools organized by what they do:

- **GitHub overview** - activity feed and summary stats
- **GitHub lists** - PRs, issues, commits, reviews, comments (all with date filters)
- **GitHub details** - drill into specific PRs or issues
- **Slack** - fetch team standups, publish, confirm before posting
- **Utilities** - clipboard, file save
- **Feedback** - capture quality ratings and comments

Here's the actual agent definition:

```python
def create_standup_agent(
    model: str = DEFAULT_MODEL,
    hooks: AgentHooks[StandupContext] | None = None,
    style_instructions: str | None = None,
) -> Agent[StandupContext]:
    instructions = _make_dynamic_instructions(style_instructions)

    return Agent[StandupContext](
        name="Standup Agent",
        instructions=instructions,
        tools=[
            # GitHub overview tools
            get_activity_feed,
            get_activity_summary,
            # GitHub list tools (with date filters)
            list_prs,
            list_issues,
            list_commits,
            list_reviews,
            list_comments,
            # GitHub assigned items (no date filter)
            list_assigned_items,
            # GitHub detail tools
            get_pr_details,
            get_issue_details,
            # Slack tools
            get_team_slack_standups,
            publish_standup_to_slack,
            confirm_slack_publish,
            set_slack_thread,
            # Utility tools
            copy_to_clipboard,
            save_standup_to_file,
            # Feedback tools
            capture_feedback_rating,
            capture_feedback_text,
        ],
        model=model,
        model_settings=ModelSettings(),
        hooks=hooks,
    )
```

The agent uses **dynamic instructions** that inject the current standup draft, so each refinement iteration has full context without blowing the token budget. A `StandupContext` object tracks collected data across tool calls without stuffing it all into the prompt.

## Stealing my own GitHub activity

The data gathering is powered by the `gh` CLI, which means:
- No API tokens to manage (just `gh auth login`)
- Access to everything you can see on GitHub
- Real-time data, not some stale cache

Here's what the activity feed looks like when the agent queries it:

```python
@function_tool
def get_activity_feed(
    ctx: RunContextWrapper[StandupContext],
    days_back: int = 1,
) -> str:
    """Get chronological feed of ALL GitHub activity."""
    # Fetches PushEvents, PullRequestEvents, ReviewEvents,
    # IssuesEvents, IssueCommentEvents...
```

The agent starts with the activity feed to get an overview, then drills down into specific PRs or issues that need more context. It's smart about not fetching redundant data and caches results so subsequent questions don't hit GitHub again.

Example of what it collects:

```
ðŸ“Š GitHub Activity Summary (last 1 day):
- 3 PRs opened/updated
- 2 code reviews given
- 5 commits pushed
- 1 issue commented on

Details:
â€¢ PR #1234: "Add LLM tracing support" - merged
â€¢ PR #1235: "Fix memory leak in agent runner" - in review
â€¢ Review on PR #1200: Approved with comments
...
```

## The secret sauce: PostHog LLM analytics

Here's where it gets interesting. Building an agent is one thing. Understanding what it's actually doing? That's where [PostHog LLM analytics](/docs/ai-engineering) comes in.

I instrumented the entire agent with our OpenAI Agents SDK integration - which, fun fact, was built during a one-day hackathon at a team onsite in Barcelona (where I also discovered [CalÃ§ots](https://en.wikipedia.org/wiki/Cal%C3%A7ot) - highly recommend). The integration was surprisingly quick to build thanks to the [excellent tracing support](https://openai.github.io/openai-agents-python/tracing/) that OpenAI baked into the Agents SDK. We just needed to extend their `TracingProcessor` to send spans to PostHog instead of their default backend.

![The team enjoying CalÃ§ots in Barcelona - fuel for hackathons](https://res.cloudinary.com/dmukukwp6/image/upload/q_auto,f_auto/team_calcots_295b5fff5e.jpg)

```python
from posthog import Posthog
from posthog.ai.openai_agents import instrument

posthog_client = Posthog(
    api_key=os.getenv("POSTHOG_API_KEY"),
    host="https://us.i.posthog.com",
)

# This single line instruments the entire agent workflow
processor = instrument(
    client=posthog_client,
    distinct_id=github_username,
    privacy_mode=False,  # I want to see everything
    properties={"app": "github-standup-agent"},
)
```

That's it. One function call and suddenly I have full visibility into everything:

### Tracing

Every agent run creates a trace with spans for:
- **Agent execution** - which agent ran and for how long
- **Tool calls** - what tools were called, with what arguments, and what they returned
- **LLM generations** - prompts, completions, token usage, and latency

I can see exactly how the agent decided which tools to call, what data it gathered, and how it turned all of that into prose.

![Trace view showing the full agent workflow](https://res.cloudinary.com/dmukukwp6/image/upload/q_auto,f_auto/trace_view_ecd9c7e1ce.png)

Each trace also gets an AI-generated summary so you can quickly understand what happened without digging into every span:

![Trace summary giving a quick overview](https://res.cloudinary.com/dmukukwp6/image/upload/q_auto,f_auto/trace_summary_3ac40f38d4.png)

### Events and metrics

Beyond tracing, I capture custom events for deeper analytics:

```python
posthog.capture(
    distinct_id=user_id,
    event="standup_generated",
    properties={
        "summary_length": len(summary),
        "days_back": days_back,
        "prs_collected": len(context.collected_prs),
        "issues_collected": len(context.collected_issues),
        "mode": "generate",  # vs "chat"
    }
)
```

Now I can answer questions like:
- How long are my standups on average?
- Do I generate more on certain days?
- How often do I use chat mode vs one-shot generation?

### Evals and feedback

The killer feature: I can rate my standups and link that feedback to the trace.

```python
@function_tool
def capture_feedback_rating(
    ctx: RunContextWrapper[StandupContext],
    rating: Literal["thumbs_up", "thumbs_down"],
    comment: str = "",
) -> str:
    """Rate the quality of this standup."""
    posthog.capture(
        event="$ai_metric",
        properties={
            "$ai_trace_id": current_trace_id,
            "$ai_metric_name": "standup_quality",
            "$ai_metric_value": 1 if rating == "thumbs_up" else 0,
            "comment": comment,
        }
    )
```

After using the agent, I can say "thumbs up, good summary" or "thumbs down, missed the important PR." That feedback is linked to the exact trace, so I can go back and see what went wrong.

Over time, this builds a dataset for understanding:
- What makes a good standup?
- Which prompts produce better results?
- Are certain types of activity harder to summarize?

This is the foundation for [evals](/docs/ai-engineering/evals) - systematic quality measurement for your AI features.

![Evals in PostHog showing quality metrics linked to traces](https://res.cloudinary.com/dmukukwp6/image/upload/q_auto,f_auto/eval_example_dfd408a6b2.png)

## The victory lap: posting to Slack

The final piece is actually posting to Slack. Because what's the point of automating standup generation if I still have to copy-paste?

The agent has Slack integration with a two-step confirmation flow (because posting to your team's Slack channel by accident would be... bad):

```
ðŸ¤–: Here's your draft standup:

## What I did
- Merged PR #1234: Add LLM tracing support
- Reviewed PR #1200: Database optimization
- Fixed memory leak in agent runner (PR #1235)

## What I'm doing
- Continuing work on the agent runner improvements
- Starting the blog post about this project

Ready to post to #team-standup? Say "yes" to confirm.

You: yes

ðŸ¤–: âœ… Posted to #team-standup!
```

The agent automatically finds the latest standup thread in your configured channel and posts as a reply. No more hunting for the thread. No more copy-paste. Just vibes.

## A real example: today's standup

Let me show you what this actually looks like in practice. Here's my actual standup session from today:

```
$ standup chat

You: lets make a standup for today

Agent [Standup Agent] starting...
  Tool: get_activity_feed(days_back=1)...
  Tool: get_activity_summary(days_back=1)...
  Tool: get_team_slack_standups(days_back=4)...
  Tool: list_prs(filter_by='authored', username='andrewm4894', state='all', days_back=1)...
  Tool: list_reviews(filter_by='given', username='andrewm4894', days_back=1)...
  Tool: list_comments(username='andrewm4894', days_back=1)...
  Tool: list_issues(filter_by='assigned', username='andrewm4894', state='all', days_back=1)...
```

The agent fires off 7 parallel tool calls to collect everything I did. It found 84 GitHub events, 31 PRs I was involved with, 17 reviews given, and even pulled in recent standups from the team Slack channel for context.

Then it drills down into specific PRs that look important:

```
  Tool: get_pr_details(repo='PostHog/posthog', number=46825)...
  Tool: get_pr_details(repo='PostHog/posthog', number=46793)...
  Tool: get_pr_details(repo='PostHog/posthog', number=46820)...
Agent [Standup Agent] finished
```

And out comes a draft standup. But here's where it gets fun - I can iterate on it naturally:

```
You: add a will do to talk to infra and data stack teams about the ml
     research platform rfc (sagemaker)

Agent [Standup Agent] starting...
Agent [Standup Agent] finished
```

The agent understood my vague instruction and added a "Will do" bullet about syncing with infra + data stack teams on the ML research platform RFC. No tool calls needed - it already had the context from the initial data gathering and just updated the draft using its dynamic instructions.

I kept iterating:

```
You: add a did about adding ai_product breakout to dashboard in llm project 2

Agent [Standup Agent] starting...
Agent [Standup Agent] finished
```

Each refinement is instant because the current standup draft is injected into the agent's instructions. It's not re-fetching data, it's just editing prose. That's the beauty of having one agent that holds all the context.

### The inferred feedback (this is the cool part)

When I was happy with the result, I said:

```
You: ok great, lets publish to slack
```

Watch what happens next:

```
Agent [Standup Agent] starting...
  Tool: capture_feedback_rating(rating='good',
        comment='User happy with updated standup and wants to publish to Slack.')...
  Tool: publish_standup_to_slack(confirmed=False)...
```

The agent **automatically inferred positive feedback** from my casual message and logged it! I didn't have to explicitly say "thumbs up" - it picked up on "ok great" and captured that as a quality signal.

This feedback gets linked to the trace in PostHog, so over time I'm building a dataset of what makes a good standup vs a bad one. That's the foundation for real evals.

### The two-step publish

The agent showed me a preview and asked for confirmation:

```
Assistant: Captured the positive feedback â€” thanks.

Slack preview is ready (channel #team-llm-analytics, thread 1770217207.394779).
Reply "yes, publish to slack" (or "confirm publish") and I'll post it.

You: yes

Agent [Standup Agent] starting...
  Tool: confirm_slack_publish()...
  Tool: publish_standup_to_slack(confirmed=True)...
```

And boom - posted to Slack:

![The final standup posted to Slack](https://res.cloudinary.com/dmukukwp6/image/upload/q_auto,f_auto/standup_bot_output_f9eff991f3.png)

The whole session took about 7 minutes, but most of that was me iterating on the draft. The actual agent work? Seconds.

Did I save any time compared to just writing the standup myself? Debatable. Did I get to use AI instead of doing it manually? Yes. And isn't that all that really matters?

<CalloutBox icon="IconBolt" title="ðŸ”¥ Hot take" type="action">

**Human in the loop still matters.** Here's the thing - as soon as I stop manually iterating and approving the standups my agent is generating, it flips into something weirdly anti-social. I'm just sending my teammates auto-generated text. Even if it's 90% accurate and I would have approved it anyway, something feels *off* about fully automating communication with other humans. The approval step isn't just quality control - it's me taking ownership of my words.

</CalloutBox>

## Cool features I'm proud of

### Make it yours with config files

The agent reads two optional config files from `~/.config/github-standup-agent/` that let you fully customize how it writes standups:

**`style.md`** - Define your team's format and conventions:

```markdown
# Standup Style Guide

## Format
- Use "Did:" and "Will Do:" sections (NOT "Yesterday/Today")
- No "Blockers" section unless explicitly needed
- Keep everything as bullet points

## Content Style
- Be concise but include enough context for team visibility
- Reference PRs with actual URLs, not just "- pr"
- Use casual/technical tone - not overly formal

## What to Skip
- Routine meetings unless notable outcome
- Minor housekeeping unless significant
```

**`examples.md`** - Provide few-shot examples from real standups:

```markdown
# Example Standups

## Andy's Example
Did:
- docs pr about verifying deployments - <link|runbooks#340>
- bugfix for lazy import breaking deploy - <link|posthog#45050>

Will Do:
- merge clustering frontend PR and watch it in prod
- follow up on support tickets I'm assigned
```

The agent uses these files to match your team's exact format, tone, and level of detail. It'll even pick up on quirks like how you reference PRs, whether you use Slack-style links, and what sections you include. No more standups that sound like a robot wrote them.

### Output guardrails

The agent has PII detection that warns you before posting anything with:
- Email addresses
- Phone numbers
- API keys or tokens
- Other sensitive patterns

Because accidentally posting your `OPENAI_API_KEY` in a standup would be a very expensive mistake.

## Try it yourself

The agent is open source: [github.com/andrewm4894/github-standup-agent](https://github.com/andrewm4894/github-standup-agent)

```bash
# Install
pip install github-standup-agent

# Configure
standup config

# Generate!
standup generate
```

You'll need:
- Python 3.11+
- GitHub CLI authenticated (`gh auth login`)
- OpenAI API key
- Optional: Slack bot token for posting
- Optional: PostHog API key for analytics

## Build your own instrumented agent

The broader lesson here: if you're building AI features, instrument them from day one.

PostHog's [LLM analytics](/docs/ai-engineering) gives you:
- **Tracing** - See exactly what your agents are doing
- **Cost tracking** - Know what you're spending on API calls
- **Evals** - Measure quality systematically
- **User analytics** - Understand how people use your AI features

The OpenAI Agents SDK integration is [available in posthog-python](https://github.com/PostHog/posthog-python/pull/408). Check out our other integrations for [LangChain](/docs/ai-engineering/langchain-integration), [OpenAI](/docs/ai-engineering/openai-integration), and [other frameworks](/docs/ai-engineering).

## Stick it to your standup bot

I've been using this agent daily for a few weeks now. It's not perfect - sometimes it misses context or formats things weirdly. But that's the beauty of having full observability: I can see exactly what went wrong and iterate.

<CalloutBox icon="IconBolt" title="ðŸ”¥ Hot take" type="action">

**Why are we even doing standups?** Honestly, summarization as a category feels crazy early. The fact that I have to manually aggregate my work across GitHub, Slack, Linear, and Google Docs is absurd. I should just be able to read the "headlines" of my work life each morning - a personalized digest of what matters, generated automatically from all my tools. No one has solved this yet. Maybe standups are just a symptom of our tools not talking to each other. The standup bot isn't the enemy - it's a coping mechanism for a fragmented toolchain.

</CalloutBox>

The standup bot still messages me at 6 PM. But now I just smile, run `standup generate`, and let my agent do the talking.

F*$k you, standup bot. I win.

---

*Want to see how your AI features are performing? [Try PostHog LLM analytics](/docs/ai-engineering) - it's free to get started.*
