---
date: "2025-11-21"
title: '6 learnings from 1 year of agents – PostHog AI'
author:
  - michael-matloka
rootPage: /blog
sidebar: Blog
showTitle: true
hideAnchor: true
featuredImage: >-
  https://res.cloudinary.com/dmukukwp6/image/upload/515178785_18a6e619_601a_4758_aaaa_06deb2695902_4a267826d4.jpg
featuredImageType: full
featuredImageCaption: Just knitting actionable insights out of data noodles.
category: PostHog news
tags:
- Product updates
- Startups
---

Today we launch **PostHog AI, the [AI agent](https://simonwillison.net/2025/Sep/18/agents/) built into PostHog**. A year in the making, we've gone a long way from our first chat prototype made over a hackathon. It all started with just one tool: "create trends chart" - and no real agentic capabilities.

Today, **PostHog AI has access to your data and setup via dozens of tools, and works tirelessly in a loop until it achieves the task you give it.** Like a product analyst, it creates multi-step analyses of product usage, writes SQL queries, sets up new feature flags and experiments, digs into impactful errors - and more, all across PostHog. After an extended beta period, PostHog AI already is used by thousands of users weekly.

The road here has been full of adventures. Here's what we've learned on building productive agents so far.

## 1. Watch out for the bulldozer of model improvements

The one constant of a year of building: model improvements change more than what you think at first. It's shocking how reasoning models were still experimental 12 months ago, because today reasoning is essential to PostHog AI's capabilities. Tool use has improved massively, as frontier models of November 2025 are able to make sense of complex tools with much greater reliability. We're currently using Claude Sonnet 4.5 for the core agent loop, as it hits a sweet spot of quality, speed, and cost – but even this will be out of date before you know it.

The two model-related step changes in our implementation were:

- introduction of cost-effective reasoning with o4-mini – this significantly improved and simplified creation of complex queries, especially those requiring data exploration to get right, ReAct BeGone!
- reliable use of tools with the Claude 4 family – the agent could finally be let loose to use a wider variety of tools without going off track

It remains hard to estimate what impact a model upgrade will really have! [A great post on this by Sean Goedecke](https://www.seangoedecke.com/are-new-models-good/) – it's true. Still, while not as grand as between GPT 2 and 3, progress is constant.

## 2. Agents beat workflows

Graph-style workflows seemed like the right choice for getting work done. In the GPT 4o era, calling tools in a loop with the same system prompt was a recipe for confusion, and so we've spent months chasing new graph-based ways of orchestrating tools. Just look at the architecture iterations we've cycled through. The initial one couldn't even answer questions with text, it could exclusively create queries, whatever the user input was!

![https://res.cloudinary.com/dmukukwp6/image/upload/Screenshot_2025_11_24_at_21_22_20_ddf57a0dd4.png]

The truth is, graphs are a terrible way of orchestrating free-form work. In a graph, the LLM can't self-correct and context is all too easily lost. Today, thanks to the advances in model capabilities, the PostHog AI architecture is oddly straightforward:

![https://res.cloudinary.com/dmukukwp6/image/upload/Screenshot_2025_11_24_at_21_23_25_edeef723b9.png]

By looping back, the LLM is able to execute across dozens of steps, while continuously verifying output. A hard-coded workflow may be cheap to execute, but it's all moot when it can't actually get work done.

## 3. A single loop beats subagents

Even in an agentic loop, it's tempting to organize task execution into specialized subagents. Doesn't it feel so clever to come up with these architectures? "The Widget Orchestrator will delegate work to the Widget Engineer, and then the work will be verified by the Widget Verifier, with input from the Widget Product Manager".

While the reasoning is smart, the results are… dumb. Context is everything for an LLM - when every layer of abstraction introduces loss of context, the ability to string tools together and self-correct course washes away. Subagents still have their place – when you're looking to parallelize work, that's still the way to go. What's important is that the delegated tasks are self-contained, and ideally independent.

As we've seen with Claude Code's success, incredible things come out of a single LLM loop with simple tools. God bless emergent behavior.

## 4. Wider context is key

Did I mention context matters? It really does, at every step of a task, because people define tasks in the most ambiguous ways. How would _you_ answer "where do users drop off in cfmp conversion"? It looks like a bunch of typos sneaked in - unless you know CFMP is a product offerred by the user, and then you need to understand how it fits into their world (this is a real example). What your agent needs is a `CLAUDE.md` equivalent – core context that's always attached.

Core context massively improves agent trajectories, but it's a new to-do for a user, and none us love it when a new tool creates setup work. We've found it critical for core context creation to be effortless, yet not forced. Ultimately, as all great artists do, we stole from the best – and implemented the `/init` command, snatched from Claude Code.

In PostHog AI, `/init` learns about your product and business via multi-step web search (currently using GPT 5 mini). The results form the agent's project-level memory. In the case your data doesn't contain URLs to work with, a few questions are asked directly, but it's something we try to minimize.

Context is _everywhere_ though. It lives on the web, but it lives in your Slack too, in the email inbox, in your notes app. This is one of the problems in applied AI that already have a number of solutions, but aren't _solved_ yet.

## 5. Frameworks considered harmful

Early on, we migrated from the OpenAI Python SDK to LangChain + LangGraph. Today, we absolutely wouldn't.

Remember the memes that JavaScript gets a new frontend framework every second? That's LLMs now, only LLMs evolve much faster than JS ever did, and the frameworks deliver less value.

The elegance of LiteLLM or Vercel AI crumbles when AI providers add new models or new features. Just check out the mess that web search results are, as OpenAI and Anthropic format them completely differently, while the frameworks try to maintain one facade.

But never, _ever_ store your state with an opinionated framework. Roll your own state. The orchestration layer of a framework like LangGraph – fine. Perhaps it's [obsolete now](#agents-beat-workflows), but it's just a way of calling functions. The state management layer of LangGraph – little-death that brings total obliteration. A custom format that relies on Python's pickling format is the mind-killer. These sorts of frameworks are fun at first, then a pain to migrate from.

AI may settle upon its React one day, but for now the framework wars rage on. It might just be best to stay low-level.

## 6. Evals are not nearly all you need

[Some](https://x.com/gdb/status/1733553161884127435) say evals can be all you need. For foundations models: certainly, you should curate your datasets. For agents: you should curate datasets too! We've found evals important for making meaningful changes with base confidence.

But reality is _gnarly_: for many real-world, multi-step tasks, setting up a realistically complex controlled environment is a greater challenge then building the agent itself. You can cover the expected happy paths easily with evals, but soon users will run into plenty you wouldn't have thought of.

What we've found even more important than raw evals is looking at real usage, and evaluating the agent's work ad-hoc. For this, we've been running **Traces Hour** - a weekly gathering of the PostHog AI team focused 100% on analyzing LLM traces from production, i.e. user interactions with our agent. (Plug: [PostHog LLM analytics](https://posthog.com/llm-analytics) is great for this.) Evals make the most sense when they stem from such investigations. Engineering is 10x'd when you grasp your users' experience, and this goes for building agents too.

## Up next

This is just the start. Today, it handles the core workflows across the platform. What's coming: deep research capabilities, sophisticated session analysis, proactive insights from background analysis, and tighter integration with code.

We've been using PostHog AI ourselves for months while building it. The team asks it to debug SQL queries, investigate user behavior, set up experiments, analyze errors – anything that would normally require clicking through multiple screens or writing complex queries.

Try it out: [Open PostHog](https://app.posthog.com/), click "PostHog AI" in the top right, grant AI access (requires admin permissions), run `/init`, and start asking it to do things.

PostHog AI – for builders, from builders.
