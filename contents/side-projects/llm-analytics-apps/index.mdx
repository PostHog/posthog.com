---
title: LLM Analytics Apps
description: Interactive CLI showcasing PostHog LLM analytics across 13+ AI providers
projectAuthor: PostHog Team
authorGitHub: PostHog
githubUrl: https://github.com/PostHog/llm-analytics-apps
filters:
  tags:
    - llm-analytics
    - python
    - node
    - cli
    - demo
---

A comprehensive interactive CLI tool demonstrating how to integrate PostHog's AI SDKs with various LLM providers. Available in both Python and Node.js implementations.

## What it does

LLM Analytics Apps provides a hands-on way to explore PostHog's LLM analytics capabilities across multiple AI providers:

- **13+ AI Providers**: Anthropic (with extended thinking), Google Gemini, OpenAI, LangChain, LiteLLM, Vercel AI, Mastra, and more
- **Multiple Test Modes**:
  - Chat Mode: Interactive conversation with any provider
  - Tool Call Test: Automated weather tool calling across providers
  - Message Test: Simple greeting tests
  - Image Test: Image description capabilities
  - Embeddings Test: Embedding generation (OpenAI)
  - Structured Output Test: Vercel AI structured data (Node.js only)
  - Transcription Test: Audio transcription (OpenAI only)
- **Extended Thinking**: Claude's internal reasoning process with configurable thinking budget

## How it uses PostHog LLM Analytics

The app demonstrates comprehensive PostHog integration:

- **Session Tracking**: Optional `ENABLE_AI_SESSION_ID` groups all traces from a single CLI session
- **Provider Integration**: All providers receive a shared PostHog instance and automatically track:
  - `$ai_trace_id`: Conversation/request identifier
  - `$ai_span_id`: Individual operation identifier
  - `$ai_model`: Model name used
  - `$ai_provider`: Provider name
  - `$ai_input` / `$ai_output`: Request/response data
  - `$ai_latency`: Operation duration
  - `$ai_input_tokens` / `$ai_output_tokens`: Token usage
- **Trace Generator**: Python-only utility for creating complex nested LLM trace data with pre-built templates

## Try it yourself

```bash
# Python
cd python && ./run.sh

# Node.js
cd node && ./run.sh
```

Select a mode (1-7), choose a provider (1-17), and start exploring LLM analytics in action.
